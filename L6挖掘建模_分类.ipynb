{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练集、验证集、测试集\n",
    "从L5数据预处理的基础上继续。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8999 3000 3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\64bit\\envs\\py3.6\\lib\\site-packages\\sklearn\\utils\\validation.py:444: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder,OneHotEncoder\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# sl:satisfaction_level——False:MinMaxScaler;True:StandardScaler\n",
    "# le:last_evaluation——False:MinMaxScaler;True:StandardScaler\n",
    "# npr:number_project——False:MinMaxScaler;True:StandardScaler\n",
    "# amh:average_monthly_hours——False:MinMaxScaler;True:StandardScaler\n",
    "# tsc:time_spend_company——False:MinMaxScaler;True:StandardScaler\n",
    "# wa:Work_accident——False:MinMaxScaler;True:StandardScaler\n",
    "# pl5:promotion_last_5years——False:MinMaxScaler;True:StandardScaler\n",
    "# dp:deparment——False:LabelEncoding;True:OneHotEncoding\n",
    "# slr:salary——False:LabelEncoding;True:OneHotEncoding\n",
    "# lower_d——False:NotlowerDimension\n",
    "# ld_n——to n dimensions\n",
    "def hr_preprocessing(sl=False,le=False,npr=False,amh=False,tsc=False,wa=False,pl5=False,dp=True,slr=False,\n",
    "                     lower_d=False,ld_n=1):\n",
    "    df=pd.read_csv('./data/HR.csv')\n",
    "    # 1、清洗数据\n",
    "    df=df.dropna(subset=['satisfaction_level','last_evaluation'])\n",
    "    df=df[df['satisfaction_level']<=1][df['salary']!='nme']\n",
    "    # 2、得到标注\n",
    "    label=df['left']\n",
    "    df=df.drop('left',axis=1)\n",
    "    # 3、特征选择(例子中的特征先全部保留)\n",
    "    # 4、特征处理\n",
    "    scaler_lst=[sl,le,npr,amh,tsc,wa,pl5]\n",
    "    column_lst=['satisfaction_level','last_evaluation','number_project','average_monthly_hours',\n",
    "                'time_spend_company','Work_accident','promotion_last_5years']\n",
    "    for i in range(len(scaler_lst)):\n",
    "        if not scaler_lst[i]:\n",
    "            df[column_lst[i]]=MinMaxScaler().fit_transform(df[column_lst[i]].values.reshape(-1,1)).reshape(1,-1)[0]\n",
    "        else:\n",
    "            df[column_lst[i]]=StandardScaler().fit_transform(df[column_lst[i]].values.reshape(-1,1)).reshape(1,-1)[0]\n",
    "    scaler_lst=[dp,slr]\n",
    "    column_lst=['department','salary']\n",
    "    for i in range(len(scaler_lst)):\n",
    "        if not scaler_lst[i]:\n",
    "            if column_lst[i]=='salary':\n",
    "                # 由于LabelEncoding会按照字母顺序来确定0,1,2，破坏了low,med,high的顺序，所以需要重新定义一个函数map_salary               \n",
    "                df[column_lst[i]]=[map_salary(s) for s in df[column_lst[i]].values]\n",
    "            else:\n",
    "                df[column_lst[i]]=LabelEncoder().fit_transform(df[column_lst[i]])  \n",
    "            # LabelEncoding之后，进行一下归一化处理\n",
    "            df[column_lst[i]]=MinMaxScaler().fit_transform(df[column_lst[i]].values.reshape(-1,1)).reshape(1,-1)[0]\n",
    "        else:\n",
    "            # OneHot编码，可以直接用pandas里面的get_dummies\n",
    "            df=pd.get_dummies(df,columns=[column_lst[i]])\n",
    "    if lower_d:\n",
    "        # PCA降维与标注Label无关，而LDA降维的n_components不能超过Label的类别（由于left只有0,1，故LDA只能降成1维）\n",
    "        return PCA(n_components=ld_n).fit_transform(df.values),label\n",
    "    return df,label\n",
    "\n",
    "def map_salary(s):\n",
    "    d=dict([('low',0),('medium',1),('high',2)])\n",
    "    return d.get(s,0)  # 将low,med,high分别赋值0,1,2，如果没有找到则赋值为0\n",
    "\n",
    "def hr_modeling(features,label):\n",
    "    f_v=features.values\n",
    "    l_v=label.values\n",
    "    # 先把验证集分离出来，再分割训练集和测试集。训练集、验证集、测试集之比6:2:2。\n",
    "    X_tt,X_validation,Y_tt,Y_validation=train_test_split(f_v,l_v,test_size=0.2)\n",
    "    X_train,X_test,Y_train,Y_test=train_test_split(X_tt,Y_tt,test_size=0.25)\n",
    "    print(len(X_train),len(X_validation),len(X_test))\n",
    "# print(hr_preprocessing(sl=True,le=True,ld_n=3))\n",
    "fetures,label=hr_preprocessing()\n",
    "hr_modeling(fetures,label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./image/6.3.png' width=500 /> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分类——KNN\n",
    "<img src='./image/6.3_2.png' width=500 /> \n",
    "<img src='./image/6.3_3.png' width=500 /> \n",
    "<img src='./image/6.3_4.png' width=500 /> \n",
    "此图中红色的点即为上图中的分割线。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\64bit\\envs\\py3.6\\lib\\site-packages\\sklearn\\utils\\validation.py:444: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n=3 ACC: 0.943333333333\n",
      "n=3 REC: 0.920833333333\n",
      "n=3 F-Score: 0.886363636364\n",
      "n=5 ACC: 0.933666666667\n",
      "n=5 REC: 0.894444444444\n",
      "n=5 F-Score: 0.866173503699\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder,OneHotEncoder\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import NearestNeighbors,KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score,recall_score,f1_score\n",
    "# sl:satisfaction_level——False:MinMaxScaler;True:StandardScaler\n",
    "# le:last_evaluation——False:MinMaxScaler;True:StandardScaler\n",
    "# npr:number_project——False:MinMaxScaler;True:StandardScaler\n",
    "# amh:average_monthly_hours——False:MinMaxScaler;True:StandardScaler\n",
    "# tsc:time_spend_company——False:MinMaxScaler;True:StandardScaler\n",
    "# wa:Work_accident——False:MinMaxScaler;True:StandardScaler\n",
    "# pl5:promotion_last_5years——False:MinMaxScaler;True:StandardScaler\n",
    "# dp:deparment——False:LabelEncoding;True:OneHotEncoding\n",
    "# slr:salary——False:LabelEncoding;True:OneHotEncoding\n",
    "# lower_d——False:NotlowerDimension\n",
    "# ld_n——to n dimensions\n",
    "def hr_preprocessing(sl=False,le=False,npr=False,amh=False,tsc=False,wa=False,pl5=False,dp=True,slr=False,\n",
    "                     lower_d=False,ld_n=1):\n",
    "    df=pd.read_csv('./data/HR.csv')\n",
    "    # 1、清洗数据\n",
    "    df=df.dropna(subset=['satisfaction_level','last_evaluation'])\n",
    "    df=df[df['satisfaction_level']<=1][df['salary']!='nme']\n",
    "    # 2、得到标注\n",
    "    label=df['left']\n",
    "    df=df.drop('left',axis=1)\n",
    "    # 3、特征选择(例子中的特征先全部保留)\n",
    "    # 4、特征处理\n",
    "    scaler_lst=[sl,le,npr,amh,tsc,wa,pl5]\n",
    "    column_lst=['satisfaction_level','last_evaluation','number_project','average_monthly_hours',\n",
    "                'time_spend_company','Work_accident','promotion_last_5years']\n",
    "    for i in range(len(scaler_lst)):\n",
    "        if not scaler_lst[i]:\n",
    "            df[column_lst[i]]=MinMaxScaler().fit_transform(df[column_lst[i]].values.reshape(-1,1)).reshape(1,-1)[0]\n",
    "        else:\n",
    "            df[column_lst[i]]=StandardScaler().fit_transform(df[column_lst[i]].values.reshape(-1,1)).reshape(1,-1)[0]\n",
    "    scaler_lst=[dp,slr]\n",
    "    column_lst=['department','salary']\n",
    "    for i in range(len(scaler_lst)):\n",
    "        if not scaler_lst[i]:\n",
    "            if column_lst[i]=='salary':\n",
    "                # 由于LabelEncoding会按照字母顺序来确定0,1,2，破坏了low,med,high的顺序，所以需要重新定义一个函数map_salary               \n",
    "                df[column_lst[i]]=[map_salary(s) for s in df[column_lst[i]].values]\n",
    "            else:\n",
    "                df[column_lst[i]]=LabelEncoder().fit_transform(df[column_lst[i]])  \n",
    "            # LabelEncoding之后，进行一下归一化处理\n",
    "            df[column_lst[i]]=MinMaxScaler().fit_transform(df[column_lst[i]].values.reshape(-1,1)).reshape(1,-1)[0]\n",
    "        else:\n",
    "            # OneHot编码，可以直接用pandas里面的get_dummies\n",
    "            df=pd.get_dummies(df,columns=[column_lst[i]])\n",
    "    if lower_d:\n",
    "        # PCA降维与标注Label无关，而LDA降维的n_components不能超过Label的类别（由于left只有0,1，故LDA只能降成1维）\n",
    "        return PCA(n_components=ld_n).fit_transform(df.values),label\n",
    "    return df,label\n",
    "\n",
    "def map_salary(s):\n",
    "    d=dict([('low',0),('medium',1),('high',2)])\n",
    "    return d.get(s,0)  # 将low,med,high分别赋值0,1,2，如果没有找到则赋值为0\n",
    "\n",
    "def hr_modeling(features,label):\n",
    "    f_v=features.values\n",
    "    l_v=label.values\n",
    "    # 先把验证集分离出来，再分割训练集和测试集。训练集、验证集、测试集之比6:2:2。\n",
    "    X_tt,X_validation,Y_tt,Y_validation=train_test_split(f_v,l_v,test_size=0.2)\n",
    "    X_train,X_test,Y_train,Y_test=train_test_split(X_tt,Y_tt,test_size=0.25)\n",
    "\n",
    "    # KNN\n",
    "    knn_clf=KNeighborsClassifier(n_neighbors=3)\n",
    "    knn_clf_n5=KNeighborsClassifier(n_neighbors=5)\n",
    "    knn_clf.fit(X_train,Y_train)\n",
    "    knn_clf_n5.fit(X_train,Y_train)\n",
    "    Y_pred=knn_clf.predict(X_validation)\n",
    "    Y_pred_n5=knn_clf_n5.predict(X_validation)\n",
    "    print('n=3 ACC:',accuracy_score(Y_validation,Y_pred))\n",
    "    print('n=3 REC:',recall_score(Y_validation,Y_pred))\n",
    "    print('n=3 F-Score:',f1_score(Y_validation,Y_pred))\n",
    "    print('n=5 ACC:',accuracy_score(Y_validation,Y_pred_n5))\n",
    "    print('n=5 REC:',recall_score(Y_validation,Y_pred_n5))\n",
    "    print('n=5 F-Score:',f1_score(Y_validation,Y_pred_n5))\n",
    "# print(hr_preprocessing(sl=True,le=True,ld_n=3))\n",
    "fetures,label=hr_preprocessing()\n",
    "hr_modeling(fetures,label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "发现n=3的效果比n=5效果好，因此将n定为3。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\64bit\\envs\\py3.6\\lib\\site-packages\\sklearn\\utils\\validation.py:444: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation set:\n",
      "ACC: 0.945333333333\n",
      "REC: 0.912181303116\n",
      "F-Score: 0.887052341598\n",
      "test set:\n",
      "ACC: 0.938\n",
      "REC: 0.895977808599\n",
      "F-Score: 0.874154262517\n",
      "train set:\n",
      "ACC: 0.971996888543\n",
      "REC: 0.958955223881\n",
      "F-Score: 0.942254812099\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder,OneHotEncoder\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import NearestNeighbors,KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score,recall_score,f1_score\n",
    "# sl:satisfaction_level——False:MinMaxScaler;True:StandardScaler\n",
    "# le:last_evaluation——False:MinMaxScaler;True:StandardScaler\n",
    "# npr:number_project——False:MinMaxScaler;True:StandardScaler\n",
    "# amh:average_monthly_hours——False:MinMaxScaler;True:StandardScaler\n",
    "# tsc:time_spend_company——False:MinMaxScaler;True:StandardScaler\n",
    "# wa:Work_accident——False:MinMaxScaler;True:StandardScaler\n",
    "# pl5:promotion_last_5years——False:MinMaxScaler;True:StandardScaler\n",
    "# dp:deparment——False:LabelEncoding;True:OneHotEncoding\n",
    "# slr:salary——False:LabelEncoding;True:OneHotEncoding\n",
    "# lower_d——False:NotlowerDimension\n",
    "# ld_n——to n dimensions\n",
    "def hr_preprocessing(sl=False,le=False,npr=False,amh=False,tsc=False,wa=False,pl5=False,dp=True,slr=False,\n",
    "                     lower_d=False,ld_n=1):\n",
    "    df=pd.read_csv('./data/HR.csv')\n",
    "    # 1、清洗数据\n",
    "    df=df.dropna(subset=['satisfaction_level','last_evaluation'])\n",
    "    df=df[df['satisfaction_level']<=1][df['salary']!='nme']\n",
    "    # 2、得到标注\n",
    "    label=df['left']\n",
    "    df=df.drop('left',axis=1)\n",
    "    # 3、特征选择(例子中的特征先全部保留)\n",
    "    # 4、特征处理\n",
    "    scaler_lst=[sl,le,npr,amh,tsc,wa,pl5]\n",
    "    column_lst=['satisfaction_level','last_evaluation','number_project','average_monthly_hours',\n",
    "                'time_spend_company','Work_accident','promotion_last_5years']\n",
    "    for i in range(len(scaler_lst)):\n",
    "        if not scaler_lst[i]:\n",
    "            df[column_lst[i]]=MinMaxScaler().fit_transform(df[column_lst[i]].values.reshape(-1,1)).reshape(1,-1)[0]\n",
    "        else:\n",
    "            df[column_lst[i]]=StandardScaler().fit_transform(df[column_lst[i]].values.reshape(-1,1)).reshape(1,-1)[0]\n",
    "    scaler_lst=[dp,slr]\n",
    "    column_lst=['department','salary']\n",
    "    for i in range(len(scaler_lst)):\n",
    "        if not scaler_lst[i]:\n",
    "            if column_lst[i]=='salary':\n",
    "                # 由于LabelEncoding会按照字母顺序来确定0,1,2，破坏了low,med,high的顺序，所以需要重新定义一个函数map_salary               \n",
    "                df[column_lst[i]]=[map_salary(s) for s in df[column_lst[i]].values]\n",
    "            else:\n",
    "                df[column_lst[i]]=LabelEncoder().fit_transform(df[column_lst[i]])  \n",
    "            # LabelEncoding之后，进行一下归一化处理\n",
    "            df[column_lst[i]]=MinMaxScaler().fit_transform(df[column_lst[i]].values.reshape(-1,1)).reshape(1,-1)[0]\n",
    "        else:\n",
    "            # OneHot编码，可以直接用pandas里面的get_dummies\n",
    "            df=pd.get_dummies(df,columns=[column_lst[i]])\n",
    "    if lower_d:\n",
    "        # PCA降维与标注Label无关，而LDA降维的n_components不能超过Label的类别（由于left只有0,1，故LDA只能降成1维）\n",
    "        return PCA(n_components=ld_n).fit_transform(df.values),label\n",
    "    return df,label\n",
    "\n",
    "def map_salary(s):\n",
    "    d=dict([('low',0),('medium',1),('high',2)])\n",
    "    return d.get(s,0)  # 将low,med,high分别赋值0,1,2，如果没有找到则赋值为0\n",
    "\n",
    "def hr_modeling(features,label):\n",
    "    f_v=features.values\n",
    "    l_v=label.values\n",
    "    # 先把验证集分离出来，再分割训练集和测试集。训练集、验证集、测试集之比6:2:2。\n",
    "    X_tt,X_validation,Y_tt,Y_validation=train_test_split(f_v,l_v,test_size=0.2)\n",
    "    X_train,X_test,Y_train,Y_test=train_test_split(X_tt,Y_tt,test_size=0.25)\n",
    "\n",
    "    # KNN\n",
    "    knn_clf=KNeighborsClassifier(n_neighbors=3)\n",
    "    knn_clf.fit(X_train,Y_train)\n",
    "    Y_pred=knn_clf.predict(X_validation)\n",
    "    print('validation set:')\n",
    "    print('ACC:',accuracy_score(Y_validation,Y_pred))\n",
    "    print('REC:',recall_score(Y_validation,Y_pred))\n",
    "    print('F-Score:',f1_score(Y_validation,Y_pred))\n",
    "    Y_pred=knn_clf.predict(X_test)\n",
    "    print('test set:')\n",
    "    print('ACC:',accuracy_score(Y_test,Y_pred))\n",
    "    print('REC:',recall_score(Y_test,Y_pred))\n",
    "    print('F-Score:',f1_score(Y_test,Y_pred))\n",
    "    Y_pred=knn_clf.predict(X_train)\n",
    "    print('train set:')\n",
    "    print('ACC:',accuracy_score(Y_train,Y_pred))\n",
    "    print('REC:',recall_score(Y_train,Y_pred))\n",
    "    print('F-Score:',f1_score(Y_train,Y_pred))\n",
    "\n",
    "fetures,label=hr_preprocessing()\n",
    "hr_modeling(fetures,label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "验证集和测试集上的效果略小于在训练集上的效果，认为泛化能力还不错，只存在稍许过拟合情况。\n",
    "<br />除了尝试变化各种模型参数以达到好的模型效果，也可以尝试改变数据预处理的方式。比如指定dp用LabelEncoder就比OneHot效果好。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\64bit\\envs\\py3.6\\lib\\site-packages\\sklearn\\utils\\validation.py:444: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation set:\n",
      "ACC: 0.954\n",
      "REC: 0.936690647482\n",
      "F-Score: 0.904166666667\n",
      "test set:\n",
      "ACC: 0.951\n",
      "REC: 0.930651872399\n",
      "F-Score: 0.901276024177\n",
      "train set:\n",
      "ACC: 0.973885987332\n",
      "REC: 0.960092807425\n",
      "F-Score: 0.946261147953\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder,OneHotEncoder\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import NearestNeighbors,KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score,recall_score,f1_score\n",
    "# sl:satisfaction_level——False:MinMaxScaler;True:StandardScaler\n",
    "# le:last_evaluation——False:MinMaxScaler;True:StandardScaler\n",
    "# npr:number_project——False:MinMaxScaler;True:StandardScaler\n",
    "# amh:average_monthly_hours——False:MinMaxScaler;True:StandardScaler\n",
    "# tsc:time_spend_company——False:MinMaxScaler;True:StandardScaler\n",
    "# wa:Work_accident——False:MinMaxScaler;True:StandardScaler\n",
    "# pl5:promotion_last_5years——False:MinMaxScaler;True:StandardScaler\n",
    "# dp:deparment——False:LabelEncoding;True:OneHotEncoding\n",
    "# slr:salary——False:LabelEncoding;True:OneHotEncoding\n",
    "# lower_d——False:NotlowerDimension\n",
    "# ld_n——to n dimensions\n",
    "def hr_preprocessing(sl=False,le=False,npr=False,amh=False,tsc=False,wa=False,pl5=False,dp=True,slr=False,\n",
    "                     lower_d=False,ld_n=1):\n",
    "    df=pd.read_csv('./data/HR.csv')\n",
    "    # 1、清洗数据\n",
    "    df=df.dropna(subset=['satisfaction_level','last_evaluation'])\n",
    "    df=df[df['satisfaction_level']<=1][df['salary']!='nme']\n",
    "    # 2、得到标注\n",
    "    label=df['left']\n",
    "    df=df.drop('left',axis=1)\n",
    "    # 3、特征选择(例子中的特征先全部保留)\n",
    "    # 4、特征处理\n",
    "    scaler_lst=[sl,le,npr,amh,tsc,wa,pl5]\n",
    "    column_lst=['satisfaction_level','last_evaluation','number_project','average_monthly_hours',\n",
    "                'time_spend_company','Work_accident','promotion_last_5years']\n",
    "    for i in range(len(scaler_lst)):\n",
    "        if not scaler_lst[i]:\n",
    "            df[column_lst[i]]=MinMaxScaler().fit_transform(df[column_lst[i]].values.reshape(-1,1)).reshape(1,-1)[0]\n",
    "        else:\n",
    "            df[column_lst[i]]=StandardScaler().fit_transform(df[column_lst[i]].values.reshape(-1,1)).reshape(1,-1)[0]\n",
    "    scaler_lst=[dp,slr]\n",
    "    column_lst=['department','salary']\n",
    "    for i in range(len(scaler_lst)):\n",
    "        if not scaler_lst[i]:\n",
    "            if column_lst[i]=='salary':\n",
    "                # 由于LabelEncoding会按照字母顺序来确定0,1,2，破坏了low,med,high的顺序，所以需要重新定义一个函数map_salary               \n",
    "                df[column_lst[i]]=[map_salary(s) for s in df[column_lst[i]].values]\n",
    "            else:\n",
    "                df[column_lst[i]]=LabelEncoder().fit_transform(df[column_lst[i]])  \n",
    "            # LabelEncoding之后，进行一下归一化处理\n",
    "            df[column_lst[i]]=MinMaxScaler().fit_transform(df[column_lst[i]].values.reshape(-1,1)).reshape(1,-1)[0]\n",
    "        else:\n",
    "            # OneHot编码，可以直接用pandas里面的get_dummies\n",
    "            df=pd.get_dummies(df,columns=[column_lst[i]])\n",
    "    if lower_d:\n",
    "        # PCA降维与标注Label无关，而LDA降维的n_components不能超过Label的类别（由于left只有0,1，故LDA只能降成1维）\n",
    "        return PCA(n_components=ld_n).fit_transform(df.values),label\n",
    "    return df,label\n",
    "\n",
    "def map_salary(s):\n",
    "    d=dict([('low',0),('medium',1),('high',2)])\n",
    "    return d.get(s,0)  # 将low,med,high分别赋值0,1,2，如果没有找到则赋值为0\n",
    "\n",
    "def hr_modeling(features,label):\n",
    "    f_v=features.values\n",
    "    l_v=label.values\n",
    "    # 先把验证集分离出来，再分割训练集和测试集。训练集、验证集、测试集之比6:2:2。\n",
    "    X_tt,X_validation,Y_tt,Y_validation=train_test_split(f_v,l_v,test_size=0.2)\n",
    "    X_train,X_test,Y_train,Y_test=train_test_split(X_tt,Y_tt,test_size=0.25)\n",
    "\n",
    "    # KNN\n",
    "    knn_clf=KNeighborsClassifier(n_neighbors=3)\n",
    "    knn_clf.fit(X_train,Y_train)\n",
    "    Y_pred=knn_clf.predict(X_validation)\n",
    "    print('validation set:')\n",
    "    print('ACC:',accuracy_score(Y_validation,Y_pred))\n",
    "    print('REC:',recall_score(Y_validation,Y_pred))\n",
    "    print('F-Score:',f1_score(Y_validation,Y_pred))\n",
    "    Y_pred=knn_clf.predict(X_test)\n",
    "    print('test set:')\n",
    "    print('ACC:',accuracy_score(Y_test,Y_pred))\n",
    "    print('REC:',recall_score(Y_test,Y_pred))\n",
    "    print('F-Score:',f1_score(Y_test,Y_pred))\n",
    "    Y_pred=knn_clf.predict(X_train)\n",
    "    print('train set:')\n",
    "    print('ACC:',accuracy_score(Y_train,Y_pred))\n",
    "    print('REC:',recall_score(Y_train,Y_pred))\n",
    "    print('F-Score:',f1_score(Y_train,Y_pred))\n",
    "\n",
    "fetures,label=hr_preprocessing(dp=False)\n",
    "hr_modeling(fetures,label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后保存训练出的模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder,OneHotEncoder\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import NearestNeighbors,KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score,recall_score,f1_score\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "# sl:satisfaction_level——False:MinMaxScaler;True:StandardScaler\n",
    "# le:last_evaluation——False:MinMaxScaler;True:StandardScaler\n",
    "# npr:number_project——False:MinMaxScaler;True:StandardScaler\n",
    "# amh:average_monthly_hours——False:MinMaxScaler;True:StandardScaler\n",
    "# tsc:time_spend_company——False:MinMaxScaler;True:StandardScaler\n",
    "# wa:Work_accident——False:MinMaxScaler;True:StandardScaler\n",
    "# pl5:promotion_last_5years——False:MinMaxScaler;True:StandardScaler\n",
    "# dp:deparment——False:LabelEncoding;True:OneHotEncoding\n",
    "# slr:salary——False:LabelEncoding;True:OneHotEncoding\n",
    "# lower_d——False:NotlowerDimension\n",
    "# ld_n——to n dimensions\n",
    "def hr_preprocessing(sl=False,le=False,npr=False,amh=False,tsc=False,wa=False,pl5=False,dp=True,slr=False,\n",
    "                     lower_d=False,ld_n=1):\n",
    "    df=pd.read_csv('./data/HR.csv')\n",
    "    # 1、清洗数据\n",
    "    df=df.dropna(subset=['satisfaction_level','last_evaluation'])\n",
    "    df=df[df['satisfaction_level']<=1][df['salary']!='nme']\n",
    "    # 2、得到标注\n",
    "    label=df['left']\n",
    "    df=df.drop('left',axis=1)\n",
    "    # 3、特征选择(例子中的特征先全部保留)\n",
    "    # 4、特征处理\n",
    "    scaler_lst=[sl,le,npr,amh,tsc,wa,pl5]\n",
    "    column_lst=['satisfaction_level','last_evaluation','number_project','average_monthly_hours',\n",
    "                'time_spend_company','Work_accident','promotion_last_5years']\n",
    "    for i in range(len(scaler_lst)):\n",
    "        if not scaler_lst[i]:\n",
    "            df[column_lst[i]]=MinMaxScaler().fit_transform(df[column_lst[i]].values.reshape(-1,1)).reshape(1,-1)[0]\n",
    "        else:\n",
    "            df[column_lst[i]]=StandardScaler().fit_transform(df[column_lst[i]].values.reshape(-1,1)).reshape(1,-1)[0]\n",
    "    scaler_lst=[dp,slr]\n",
    "    column_lst=['department','salary']\n",
    "    for i in range(len(scaler_lst)):\n",
    "        if not scaler_lst[i]:\n",
    "            if column_lst[i]=='salary':\n",
    "                # 由于LabelEncoding会按照字母顺序来确定0,1,2，破坏了low,med,high的顺序，所以需要重新定义一个函数map_salary               \n",
    "                df[column_lst[i]]=[map_salary(s) for s in df[column_lst[i]].values]\n",
    "            else:\n",
    "                df[column_lst[i]]=LabelEncoder().fit_transform(df[column_lst[i]])  \n",
    "            # LabelEncoding之后，进行一下归一化处理\n",
    "            df[column_lst[i]]=MinMaxScaler().fit_transform(df[column_lst[i]].values.reshape(-1,1)).reshape(1,-1)[0]\n",
    "        else:\n",
    "            # OneHot编码，可以直接用pandas里面的get_dummies\n",
    "            df=pd.get_dummies(df,columns=[column_lst[i]])\n",
    "    if lower_d:\n",
    "        # PCA降维与标注Label无关，而LDA降维的n_components不能超过Label的类别（由于left只有0,1，故LDA只能降成1维）\n",
    "        return PCA(n_components=ld_n).fit_transform(df.values),label\n",
    "    return df,label\n",
    "\n",
    "def map_salary(s):\n",
    "    d=dict([('low',0),('medium',1),('high',2)])\n",
    "    return d.get(s,0)  # 将low,med,high分别赋值0,1,2，如果没有找到则赋值为0\n",
    "\n",
    "def hr_modeling(features,label):\n",
    "    f_v=features.values\n",
    "    l_v=label.values\n",
    "    # 先把验证集分离出来，再分割训练集和测试集。训练集、验证集、测试集之比6:2:2。\n",
    "    X_tt,X_validation,Y_tt,Y_validation=train_test_split(f_v,l_v,test_size=0.2)\n",
    "    X_train,X_test,Y_train,Y_test=train_test_split(X_tt,Y_tt,test_size=0.25)\n",
    "\n",
    "    # KNN\n",
    "    knn_clf=KNeighborsClassifier(n_neighbors=3)\n",
    "    knn_clf.fit(X_train,Y_train)\n",
    "    Y_pred=knn_clf.predict(X_validation)\n",
    "    print('validation set:')\n",
    "    print('ACC:',accuracy_score(Y_validation,Y_pred))\n",
    "    print('REC:',recall_score(Y_validation,Y_pred))\n",
    "    print('F-Score:',f1_score(Y_validation,Y_pred))\n",
    "    Y_pred=knn_clf.predict(X_test)\n",
    "    print('test set:')\n",
    "    print('ACC:',accuracy_score(Y_test,Y_pred))\n",
    "    print('REC:',recall_score(Y_test,Y_pred))\n",
    "    print('F-Score:',f1_score(Y_test,Y_pred))\n",
    "    Y_pred=knn_clf.predict(X_train)\n",
    "    print('train set:')\n",
    "    print('ACC:',accuracy_score(Y_train,Y_pred))\n",
    "    print('REC:',recall_score(Y_train,Y_pred))\n",
    "    print('F-Score:',f1_score(Y_train,Y_pred))\n",
    "    \n",
    "    joblib.dump(knn_clf,'knn_clf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上这一段在PyCharm中跑一遍就会在Project中添加一个名为‘knn_clf’的文件。\n",
    "<br />之后，引用该模型则可以用knn_clf=joblib.load('knn_clf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\64bit\\envs\\py3.6\\lib\\site-packages\\sklearn\\utils\\validation.py:444: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "D:\\anaconda\\64bit\\envs\\py3.6\\lib\\site-packages\\sklearn\\base.py:312: UserWarning: Trying to unpickle estimator KNeighborsClassifier from version 0.19.1 when using version 0.19.0. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation set:\n",
      "ACC: 0.964\n",
      "REC: 0.944444444444\n",
      "F-Score: 0.922857142857\n",
      "test set:\n",
      "ACC: 0.964333333333\n",
      "REC: 0.95567867036\n",
      "F-Score: 0.928043039677\n",
      "train set:\n",
      "ACC: 0.966885209468\n",
      "REC: 0.944110854503\n",
      "F-Score: 0.932056543548\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder,OneHotEncoder\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import NearestNeighbors,KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score,recall_score,f1_score\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "# sl:satisfaction_level——False:MinMaxScaler;True:StandardScaler\n",
    "# le:last_evaluation——False:MinMaxScaler;True:StandardScaler\n",
    "# npr:number_project——False:MinMaxScaler;True:StandardScaler\n",
    "# amh:average_monthly_hours——False:MinMaxScaler;True:StandardScaler\n",
    "# tsc:time_spend_company——False:MinMaxScaler;True:StandardScaler\n",
    "# wa:Work_accident——False:MinMaxScaler;True:StandardScaler\n",
    "# pl5:promotion_last_5years——False:MinMaxScaler;True:StandardScaler\n",
    "# dp:deparment——False:LabelEncoding;True:OneHotEncoding\n",
    "# slr:salary——False:LabelEncoding;True:OneHotEncoding\n",
    "# lower_d——False:NotlowerDimension\n",
    "# ld_n——to n dimensions\n",
    "def hr_preprocessing(sl=False,le=False,npr=False,amh=False,tsc=False,wa=False,pl5=False,dp=True,slr=False,\n",
    "                     lower_d=False,ld_n=1):\n",
    "    df=pd.read_csv('./data/HR.csv')\n",
    "    # 1、清洗数据\n",
    "    df=df.dropna(subset=['satisfaction_level','last_evaluation'])\n",
    "    df=df[df['satisfaction_level']<=1][df['salary']!='nme']\n",
    "    # 2、得到标注\n",
    "    label=df['left']\n",
    "    df=df.drop('left',axis=1)\n",
    "    # 3、特征选择(例子中的特征先全部保留)\n",
    "    # 4、特征处理\n",
    "    scaler_lst=[sl,le,npr,amh,tsc,wa,pl5]\n",
    "    column_lst=['satisfaction_level','last_evaluation','number_project','average_monthly_hours',\n",
    "                'time_spend_company','Work_accident','promotion_last_5years']\n",
    "    for i in range(len(scaler_lst)):\n",
    "        if not scaler_lst[i]:\n",
    "            df[column_lst[i]]=MinMaxScaler().fit_transform(df[column_lst[i]].values.reshape(-1,1)).reshape(1,-1)[0]\n",
    "        else:\n",
    "            df[column_lst[i]]=StandardScaler().fit_transform(df[column_lst[i]].values.reshape(-1,1)).reshape(1,-1)[0]\n",
    "    scaler_lst=[dp,slr]\n",
    "    column_lst=['department','salary']\n",
    "    for i in range(len(scaler_lst)):\n",
    "        if not scaler_lst[i]:\n",
    "            if column_lst[i]=='salary':\n",
    "                # 由于LabelEncoding会按照字母顺序来确定0,1,2，破坏了low,med,high的顺序，所以需要重新定义一个函数map_salary               \n",
    "                df[column_lst[i]]=[map_salary(s) for s in df[column_lst[i]].values]\n",
    "            else:\n",
    "                df[column_lst[i]]=LabelEncoder().fit_transform(df[column_lst[i]])  \n",
    "            # LabelEncoding之后，进行一下归一化处理\n",
    "            df[column_lst[i]]=MinMaxScaler().fit_transform(df[column_lst[i]].values.reshape(-1,1)).reshape(1,-1)[0]\n",
    "        else:\n",
    "            # OneHot编码，可以直接用pandas里面的get_dummies\n",
    "            df=pd.get_dummies(df,columns=[column_lst[i]])\n",
    "    if lower_d:\n",
    "        # PCA降维与标注Label无关，而LDA降维的n_components不能超过Label的类别（由于left只有0,1，故LDA只能降成1维）\n",
    "        return PCA(n_components=ld_n).fit_transform(df.values),label\n",
    "    return df,label\n",
    "\n",
    "def map_salary(s):\n",
    "    d=dict([('low',0),('medium',1),('high',2)])\n",
    "    return d.get(s,0)  # 将low,med,high分别赋值0,1,2，如果没有找到则赋值为0\n",
    "\n",
    "def hr_modeling(features,label):\n",
    "    f_v=features.values\n",
    "    l_v=label.values\n",
    "    # 先把验证集分离出来，再分割训练集和测试集。训练集、验证集、测试集之比6:2:2。\n",
    "    X_tt,X_validation,Y_tt,Y_validation=train_test_split(f_v,l_v,test_size=0.2)\n",
    "    X_train,X_test,Y_train,Y_test=train_test_split(X_tt,Y_tt,test_size=0.25)\n",
    "\n",
    "    # KNN\n",
    "    knn_clf=joblib.load('knn_clf')\n",
    "    Y_pred=knn_clf.predict(X_validation)\n",
    "    print('validation set:')\n",
    "    print('ACC:',accuracy_score(Y_validation,Y_pred))\n",
    "    print('REC:',recall_score(Y_validation,Y_pred))\n",
    "    print('F-Score:',f1_score(Y_validation,Y_pred))\n",
    "    Y_pred=knn_clf.predict(X_test)\n",
    "    print('test set:')\n",
    "    print('ACC:',accuracy_score(Y_test,Y_pred))\n",
    "    print('REC:',recall_score(Y_test,Y_pred))\n",
    "    print('F-Score:',f1_score(Y_test,Y_pred))\n",
    "    Y_pred=knn_clf.predict(X_train)\n",
    "    print('train set:')\n",
    "    print('ACC:',accuracy_score(Y_train,Y_pred))\n",
    "    print('REC:',recall_score(Y_train,Y_pred))\n",
    "    print('F-Score:',f1_score(Y_train,Y_pred))\n",
    "    \n",
    "fetures,label=hr_preprocessing(dp=False)\n",
    "hr_modeling(fetures,label)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分类——朴素贝叶斯\n",
    "<img src='./image/6.4.png' width=500 />\n",
    "<br />例子：\n",
    "<img src='./image/6.4_2.png' width=500 />\n",
    "<img src='./image/6.4_3.png' width=500 />\n",
    "<img src='./image/6.4_4.png' width=500 />\n",
    "<img src='./image/6.4_5.png' width=500 />\n",
    "但是如果贝叶斯公式中出现分子分母均为0的情况，如：\n",
    "<img src='./image/6.4_6.png' width=500 />\n",
    "<img src='./image/6.4_7.png' width=500 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在下一步实现朴素贝叶斯之前，现将之前的模型进行统一管理，放到models=[]里面。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\64bit\\envs\\py3.6\\lib\\site-packages\\sklearn\\utils\\validation.py:444: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- KNN --------\n",
      "Train Set\n",
      "-ACC: 0.977553061451\n",
      "-REC: 0.968377635197\n",
      "-F-Score: 0.954381210479\n",
      "Validation Set\n",
      "-ACC: 0.955666666667\n",
      "-REC: 0.934971098266\n",
      "-F-Score: 0.906797477225\n",
      "Test Set\n",
      "-ACC: 0.953\n",
      "-REC: 0.932568149211\n",
      "-F-Score: 0.902151283831\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder,OneHotEncoder\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import NearestNeighbors,KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score,recall_score,f1_score\n",
    "# sl:satisfaction_level——False:MinMaxScaler;True:StandardScaler\n",
    "# le:last_evaluation——False:MinMaxScaler;True:StandardScaler\n",
    "# npr:number_project——False:MinMaxScaler;True:StandardScaler\n",
    "# amh:average_monthly_hours——False:MinMaxScaler;True:StandardScaler\n",
    "# tsc:time_spend_company——False:MinMaxScaler;True:StandardScaler\n",
    "# wa:Work_accident——False:MinMaxScaler;True:StandardScaler\n",
    "# pl5:promotion_last_5years——False:MinMaxScaler;True:StandardScaler\n",
    "# dp:deparment——False:LabelEncoding;True:OneHotEncoding\n",
    "# slr:salary——False:LabelEncoding;True:OneHotEncoding\n",
    "# lower_d——False:NotlowerDimension\n",
    "# ld_n——to n dimensions\n",
    "def hr_preprocessing(sl=False,le=False,npr=False,amh=False,tsc=False,wa=False,pl5=False,dp=True,slr=False,\n",
    "                     lower_d=False,ld_n=1):\n",
    "    df=pd.read_csv('./data/HR.csv')\n",
    "    # 1、清洗数据\n",
    "    df=df.dropna(subset=['satisfaction_level','last_evaluation'])\n",
    "    df=df[df['satisfaction_level']<=1][df['salary']!='nme']\n",
    "    # 2、得到标注\n",
    "    label=df['left']\n",
    "    df=df.drop('left',axis=1)\n",
    "    # 3、特征选择(例子中的特征先全部保留)\n",
    "    # 4、特征处理\n",
    "    scaler_lst=[sl,le,npr,amh,tsc,wa,pl5]\n",
    "    column_lst=['satisfaction_level','last_evaluation','number_project','average_monthly_hours',\n",
    "                'time_spend_company','Work_accident','promotion_last_5years']\n",
    "    for i in range(len(scaler_lst)):\n",
    "        if not scaler_lst[i]:\n",
    "            df[column_lst[i]]=MinMaxScaler().fit_transform(df[column_lst[i]].values.reshape(-1,1)).reshape(1,-1)[0]\n",
    "        else:\n",
    "            df[column_lst[i]]=StandardScaler().fit_transform(df[column_lst[i]].values.reshape(-1,1)).reshape(1,-1)[0]\n",
    "    scaler_lst=[dp,slr]\n",
    "    column_lst=['department','salary']\n",
    "    for i in range(len(scaler_lst)):\n",
    "        if not scaler_lst[i]:\n",
    "            if column_lst[i]=='salary':\n",
    "                # 由于LabelEncoding会按照字母顺序来确定0,1,2，破坏了low,med,high的顺序，所以需要重新定义一个函数map_salary               \n",
    "                df[column_lst[i]]=[map_salary(s) for s in df[column_lst[i]].values]\n",
    "            else:\n",
    "                df[column_lst[i]]=LabelEncoder().fit_transform(df[column_lst[i]])  \n",
    "            # LabelEncoding之后，进行一下归一化处理\n",
    "            df[column_lst[i]]=MinMaxScaler().fit_transform(df[column_lst[i]].values.reshape(-1,1)).reshape(1,-1)[0]\n",
    "        else:\n",
    "            # OneHot编码，可以直接用pandas里面的get_dummies\n",
    "            df=pd.get_dummies(df,columns=[column_lst[i]])\n",
    "    if lower_d:\n",
    "        # PCA降维与标注Label无关，而LDA降维的n_components不能超过Label的类别（由于left只有0,1，故LDA只能降成1维）\n",
    "        return PCA(n_components=ld_n).fit_transform(df.values),label\n",
    "    return df,label\n",
    "\n",
    "def map_salary(s):\n",
    "    d=dict([('low',0),('medium',1),('high',2)])\n",
    "    return d.get(s,0)  # 将low,med,high分别赋值0,1,2，如果没有找到则赋值为0\n",
    "\n",
    "def hr_modeling(features,label):\n",
    "    f_v=features.values\n",
    "    l_v=label.values\n",
    "    # 先把验证集分离出来，再分割训练集和测试集。训练集、验证集、测试集之比6:2:2。\n",
    "    X_tt,X_validation,Y_tt,Y_validation=train_test_split(f_v,l_v,test_size=0.2)\n",
    "    X_train,X_test,Y_train,Y_test=train_test_split(X_tt,Y_tt,test_size=0.25)\n",
    "\n",
    "    # \n",
    "    models=[]\n",
    "    models.append(('KNN',KNeighborsClassifier(n_neighbors=3)))\n",
    "    for clf_name,clf in models:\n",
    "        clf.fit(X_train,Y_train)\n",
    "        xy_lst=[(X_train,Y_train),(X_validation,Y_validation),(X_test,Y_test)]\n",
    "        d=dict([(0,'Train Set'),(1,'Validation Set'),(2,'Test Set')])\n",
    "        print('-'*8,clf_name,'-'*8)\n",
    "        for i in range(len(xy_lst)):\n",
    "            X_part=xy_lst[i][0]\n",
    "            Y_part=xy_lst[i][1]\n",
    "            Y_pred=clf.predict(X_part)\n",
    "            print(d.get(i))\n",
    "            print('-ACC:',accuracy_score(Y_part,Y_pred))\n",
    "            print('-REC:',recall_score(Y_part,Y_pred))\n",
    "            print('-F-Score:',f1_score(Y_part,Y_pred))\n",
    "\n",
    "\n",
    "fetures,label=hr_preprocessing(dp=False)\n",
    "hr_modeling(fetures,label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加入朴素贝叶斯模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\64bit\\envs\\py3.6\\lib\\site-packages\\sklearn\\utils\\validation.py:444: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- KNN --------\n",
      "Train Set\n",
      "-ACC: 0.976552950328\n",
      "-REC: 0.969245107176\n",
      "-F-Score: 0.951727293526\n",
      "Validation Set\n",
      "-ACC: 0.951333333333\n",
      "-REC: 0.920765027322\n",
      "-F-Score: 0.902275769746\n",
      "Test Set\n",
      "-ACC: 0.950333333333\n",
      "-REC: 0.923520923521\n",
      "-F-Score: 0.895731280616\n",
      "-------- GaussianNB --------\n",
      "Train Set\n",
      "-ACC: 0.79319924436\n",
      "-REC: 0.745107176142\n",
      "-F-Score: 0.632140739277\n",
      "Validation Set\n",
      "-ACC: 0.798\n",
      "-REC: 0.743169398907\n",
      "-F-Score: 0.642266824085\n",
      "Test Set\n",
      "-ACC: 0.796666666667\n",
      "-REC: 0.756132756133\n",
      "-F-Score: 0.632086851628\n",
      "-------- BernoulliNB --------\n",
      "Train Set\n",
      "-ACC: 0.842538059784\n",
      "-REC: 0.465051258155\n",
      "-F-Score: 0.584822736595\n",
      "Validation Set\n",
      "-ACC: 0.830333333333\n",
      "-REC: 0.449453551913\n",
      "-F-Score: 0.563838903171\n",
      "Test Set\n",
      "-ACC: 0.850333333333\n",
      "-REC: 0.499278499278\n",
      "-F-Score: 0.606485539001\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder,OneHotEncoder\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import NearestNeighbors,KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB,BernoulliNB\n",
    "\n",
    "from sklearn.metrics import accuracy_score,recall_score,f1_score\n",
    "# sl:satisfaction_level——False:MinMaxScaler;True:StandardScaler\n",
    "# le:last_evaluation——False:MinMaxScaler;True:StandardScaler\n",
    "# npr:number_project——False:MinMaxScaler;True:StandardScaler\n",
    "# amh:average_monthly_hours——False:MinMaxScaler;True:StandardScaler\n",
    "# tsc:time_spend_company——False:MinMaxScaler;True:StandardScaler\n",
    "# wa:Work_accident——False:MinMaxScaler;True:StandardScaler\n",
    "# pl5:promotion_last_5years——False:MinMaxScaler;True:StandardScaler\n",
    "# dp:deparment——False:LabelEncoding;True:OneHotEncoding\n",
    "# slr:salary——False:LabelEncoding;True:OneHotEncoding\n",
    "# lower_d——False:NotlowerDimension\n",
    "# ld_n——to n dimensions\n",
    "def hr_preprocessing(sl=False,le=False,npr=False,amh=False,tsc=False,wa=False,pl5=False,dp=True,slr=False,\n",
    "                     lower_d=False,ld_n=1):\n",
    "    df=pd.read_csv('./data/HR.csv')\n",
    "    # 1、清洗数据\n",
    "    df=df.dropna(subset=['satisfaction_level','last_evaluation'])\n",
    "    df=df[df['satisfaction_level']<=1][df['salary']!='nme']\n",
    "    # 2、得到标注\n",
    "    label=df['left']\n",
    "    df=df.drop('left',axis=1)\n",
    "    # 3、特征选择(例子中的特征先全部保留)\n",
    "    # 4、特征处理\n",
    "    scaler_lst=[sl,le,npr,amh,tsc,wa,pl5]\n",
    "    column_lst=['satisfaction_level','last_evaluation','number_project','average_monthly_hours',\n",
    "                'time_spend_company','Work_accident','promotion_last_5years']\n",
    "    for i in range(len(scaler_lst)):\n",
    "        if not scaler_lst[i]:\n",
    "            df[column_lst[i]]=MinMaxScaler().fit_transform(df[column_lst[i]].values.reshape(-1,1)).reshape(1,-1)[0]\n",
    "        else:\n",
    "            df[column_lst[i]]=StandardScaler().fit_transform(df[column_lst[i]].values.reshape(-1,1)).reshape(1,-1)[0]\n",
    "    scaler_lst=[dp,slr]\n",
    "    column_lst=['department','salary']\n",
    "    for i in range(len(scaler_lst)):\n",
    "        if not scaler_lst[i]:\n",
    "            if column_lst[i]=='salary':\n",
    "                # 由于LabelEncoding会按照字母顺序来确定0,1,2，破坏了low,med,high的顺序，所以需要重新定义一个函数map_salary               \n",
    "                df[column_lst[i]]=[map_salary(s) for s in df[column_lst[i]].values]\n",
    "            else:\n",
    "                df[column_lst[i]]=LabelEncoder().fit_transform(df[column_lst[i]])  \n",
    "            # LabelEncoding之后，进行一下归一化处理\n",
    "            df[column_lst[i]]=MinMaxScaler().fit_transform(df[column_lst[i]].values.reshape(-1,1)).reshape(1,-1)[0]\n",
    "        else:\n",
    "            # OneHot编码，可以直接用pandas里面的get_dummies\n",
    "            df=pd.get_dummies(df,columns=[column_lst[i]])\n",
    "    if lower_d:\n",
    "        # PCA降维与标注Label无关，而LDA降维的n_components不能超过Label的类别（由于left只有0,1，故LDA只能降成1维）\n",
    "        return PCA(n_components=ld_n).fit_transform(df.values),label\n",
    "    return df,label\n",
    "\n",
    "def map_salary(s):\n",
    "    d=dict([('low',0),('medium',1),('high',2)])\n",
    "    return d.get(s,0)  # 将low,med,high分别赋值0,1,2，如果没有找到则赋值为0\n",
    "\n",
    "def hr_modeling(features,label):\n",
    "    f_v=features.values\n",
    "    l_v=label.values\n",
    "    # 先把验证集分离出来，再分割训练集和测试集。训练集、验证集、测试集之比6:2:2。\n",
    "    X_tt,X_validation,Y_tt,Y_validation=train_test_split(f_v,l_v,test_size=0.2)\n",
    "    X_train,X_test,Y_train,Y_test=train_test_split(X_tt,Y_tt,test_size=0.25)\n",
    "\n",
    "    # \n",
    "    models=[]\n",
    "    models.append(('KNN',KNeighborsClassifier(n_neighbors=3)))\n",
    "    \n",
    "    models.append(('GaussianNB',GaussianNB()))\n",
    "    models.append(('BernoulliNB',BernoulliNB()))\n",
    "    \n",
    "    for clf_name,clf in models:\n",
    "        clf.fit(X_train,Y_train)\n",
    "        xy_lst=[(X_train,Y_train),(X_validation,Y_validation),(X_test,Y_test)]\n",
    "        d=dict([(0,'Train Set'),(1,'Validation Set'),(2,'Test Set')])\n",
    "        print('-'*8,clf_name,'-'*8)\n",
    "        for i in range(len(xy_lst)):\n",
    "            X_part=xy_lst[i][0]\n",
    "            Y_part=xy_lst[i][1]\n",
    "            Y_pred=clf.predict(X_part)\n",
    "            print(d.get(i))\n",
    "            print('-ACC:',accuracy_score(Y_part,Y_pred))\n",
    "            print('-REC:',recall_score(Y_part,Y_pred))\n",
    "            print('-F-Score:',f1_score(Y_part,Y_pred))\n",
    "\n",
    "\n",
    "fetures,label=hr_preprocessing(dp=False)\n",
    "hr_modeling(fetures,label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "发现朴素贝叶斯分类器的效果不是很好。如果一份数据中绝大多数都是离散的，可以考虑朴素贝叶斯。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./image/6.4_8.png' width=500 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分类——决策树\n",
    "例子：\n",
    "<img src='./image/6.4_9.png' width=500 />\n",
    "<img src='./image/6.4_10.png' width=500 />\n",
    "<img src='./image/6.4_11.png' width=500 />\n",
    "<img src='./image/6.4_12.png' width=500 />\n",
    "不纯度最低的切分是很有效的切分，应该先考虑。\n",
    "<img src='./image/6.4_13.png' width=500 /> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\64bit\\envs\\py3.6\\lib\\site-packages\\sklearn\\utils\\validation.py:444: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- KNN --------\n",
      "Train Set\n",
      "-ACC: 0.974108234248\n",
      "-REC: 0.957865168539\n",
      "-F-Score: 0.94612716763\n",
      "Validation Set\n",
      "-ACC: 0.952666666667\n",
      "-REC: 0.923928077455\n",
      "-F-Score: 0.903924221922\n",
      "Test Set\n",
      "-ACC: 0.951666666667\n",
      "-REC: 0.915730337079\n",
      "-F-Score: 0.899930986888\n",
      "-------- GaussianNB --------\n",
      "Train Set\n",
      "-ACC: 0.794532725858\n",
      "-REC: 0.716760299625\n",
      "-F-Score: 0.623498269192\n",
      "Validation Set\n",
      "-ACC: 0.791666666667\n",
      "-REC: 0.723374827109\n",
      "-F-Score: 0.625972471574\n",
      "Test Set\n",
      "-ACC: 0.797\n",
      "-REC: 0.717696629213\n",
      "-F-Score: 0.62660944206\n",
      "-------- BernoulliNB --------\n",
      "Train Set\n",
      "-ACC: 0.843093677075\n",
      "-REC: 0.47893258427\n",
      "-F-Score: 0.591671486408\n",
      "Validation Set\n",
      "-ACC: 0.836\n",
      "-REC: 0.457814661134\n",
      "-F-Score: 0.573656845754\n",
      "Test Set\n",
      "-ACC: 0.843\n",
      "-REC: 0.448033707865\n",
      "-F-Score: 0.575293056808\n",
      "-------- DecisionTree --------\n",
      "Train Set\n",
      "-ACC: 1.0\n",
      "-REC: 1.0\n",
      "-F-Score: 1.0\n",
      "Validation Set\n",
      "-ACC: 0.974666666667\n",
      "-REC: 0.96265560166\n",
      "-F-Score: 0.948228882834\n",
      "Test Set\n",
      "-ACC: 0.976333333333\n",
      "-REC: 0.967696629213\n",
      "-F-Score: 0.951000690131\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder,OneHotEncoder\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import NearestNeighbors,KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB,BernoulliNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score,recall_score,f1_score\n",
    "# sl:satisfaction_level——False:MinMaxScaler;True:StandardScaler\n",
    "# le:last_evaluation——False:MinMaxScaler;True:StandardScaler\n",
    "# npr:number_project——False:MinMaxScaler;True:StandardScaler\n",
    "# amh:average_monthly_hours——False:MinMaxScaler;True:StandardScaler\n",
    "# tsc:time_spend_company——False:MinMaxScaler;True:StandardScaler\n",
    "# wa:Work_accident——False:MinMaxScaler;True:StandardScaler\n",
    "# pl5:promotion_last_5years——False:MinMaxScaler;True:StandardScaler\n",
    "# dp:deparment——False:LabelEncoding;True:OneHotEncoding\n",
    "# slr:salary——False:LabelEncoding;True:OneHotEncoding\n",
    "# lower_d——False:NotlowerDimension\n",
    "# ld_n——to n dimensions\n",
    "def hr_preprocessing(sl=False,le=False,npr=False,amh=False,tsc=False,wa=False,pl5=False,dp=True,slr=False,\n",
    "                     lower_d=False,ld_n=1):\n",
    "    df=pd.read_csv('./data/HR.csv')\n",
    "    # 1、清洗数据\n",
    "    df=df.dropna(subset=['satisfaction_level','last_evaluation'])\n",
    "    df=df[df['satisfaction_level']<=1][df['salary']!='nme']\n",
    "    # 2、得到标注\n",
    "    label=df['left']\n",
    "    df=df.drop('left',axis=1)\n",
    "    # 3、特征选择(例子中的特征先全部保留)\n",
    "    # 4、特征处理\n",
    "    scaler_lst=[sl,le,npr,amh,tsc,wa,pl5]\n",
    "    column_lst=['satisfaction_level','last_evaluation','number_project','average_monthly_hours',\n",
    "                'time_spend_company','Work_accident','promotion_last_5years']\n",
    "    for i in range(len(scaler_lst)):\n",
    "        if not scaler_lst[i]:\n",
    "            df[column_lst[i]]=MinMaxScaler().fit_transform(df[column_lst[i]].values.reshape(-1,1)).reshape(1,-1)[0]\n",
    "        else:\n",
    "            df[column_lst[i]]=StandardScaler().fit_transform(df[column_lst[i]].values.reshape(-1,1)).reshape(1,-1)[0]\n",
    "    scaler_lst=[dp,slr]\n",
    "    column_lst=['department','salary']\n",
    "    for i in range(len(scaler_lst)):\n",
    "        if not scaler_lst[i]:\n",
    "            if column_lst[i]=='salary':\n",
    "                # 由于LabelEncoding会按照字母顺序来确定0,1,2，破坏了low,med,high的顺序，所以需要重新定义一个函数map_salary               \n",
    "                df[column_lst[i]]=[map_salary(s) for s in df[column_lst[i]].values]\n",
    "            else:\n",
    "                df[column_lst[i]]=LabelEncoder().fit_transform(df[column_lst[i]])  \n",
    "            # LabelEncoding之后，进行一下归一化处理\n",
    "            df[column_lst[i]]=MinMaxScaler().fit_transform(df[column_lst[i]].values.reshape(-1,1)).reshape(1,-1)[0]\n",
    "        else:\n",
    "            # OneHot编码，可以直接用pandas里面的get_dummies\n",
    "            df=pd.get_dummies(df,columns=[column_lst[i]])\n",
    "    if lower_d:\n",
    "        # PCA降维与标注Label无关，而LDA降维的n_components不能超过Label的类别（由于left只有0,1，故LDA只能降成1维）\n",
    "        return PCA(n_components=ld_n).fit_transform(df.values),label\n",
    "    return df,label\n",
    "\n",
    "def map_salary(s):\n",
    "    d=dict([('low',0),('medium',1),('high',2)])\n",
    "    return d.get(s,0)  # 将low,med,high分别赋值0,1,2，如果没有找到则赋值为0\n",
    "\n",
    "def hr_modeling(features,label):\n",
    "    f_v=features.values\n",
    "    l_v=label.values\n",
    "    # 先把验证集分离出来，再分割训练集和测试集。训练集、验证集、测试集之比6:2:2。\n",
    "    X_tt,X_validation,Y_tt,Y_validation=train_test_split(f_v,l_v,test_size=0.2)\n",
    "    X_train,X_test,Y_train,Y_test=train_test_split(X_tt,Y_tt,test_size=0.25)\n",
    "\n",
    "    # KNN\n",
    "    models=[]\n",
    "    models.append(('KNN',KNeighborsClassifier(n_neighbors=3)))\n",
    "    # 朴素贝叶斯\n",
    "    models.append(('GaussianNB',GaussianNB()))\n",
    "    models.append(('BernoulliNB',BernoulliNB()))\n",
    "    # 决策树\n",
    "    models.append(('DecisionTree',DecisionTreeClassifier()))\n",
    "    \n",
    "    for clf_name,clf in models:\n",
    "        clf.fit(X_train,Y_train)\n",
    "        xy_lst=[(X_train,Y_train),(X_validation,Y_validation),(X_test,Y_test)]\n",
    "        d=dict([(0,'Train Set'),(1,'Validation Set'),(2,'Test Set')])\n",
    "        print('-'*8,clf_name,'-'*8)\n",
    "        for i in range(len(xy_lst)):\n",
    "            X_part=xy_lst[i][0]\n",
    "            Y_part=xy_lst[i][1]\n",
    "            Y_pred=clf.predict(X_part)\n",
    "            print(d.get(i))\n",
    "            print('-ACC:',accuracy_score(Y_part,Y_pred))\n",
    "            print('-REC:',recall_score(Y_part,Y_pred))\n",
    "            print('-F-Score:',f1_score(Y_part,Y_pred))\n",
    "\n",
    "\n",
    "fetures,label=hr_preprocessing(dp=False)\n",
    "hr_modeling(fetures,label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来希望把决策树画出来。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- DecisionTreeGini --------\n",
      "Train Set\n",
      "-ACC: 1.0\n",
      "-REC: 1.0\n",
      "-F-Score: 1.0\n",
      "Validation Set\n",
      "-ACC: 0.975333333333\n",
      "-REC: 0.965706447188\n",
      "-F-Score: 0.950067476383\n",
      "Test Set\n",
      "-ACC: 0.972333333333\n",
      "-REC: 0.951790633609\n",
      "-F-Score: 0.943344709898\n",
      "-------- DecisionTreeEntropy --------\n",
      "Train Set\n",
      "-ACC: 1.0\n",
      "-REC: 1.0\n",
      "-F-Score: 1.0\n",
      "Validation Set\n",
      "-ACC: 0.978\n",
      "-REC: 0.968449931413\n",
      "-F-Score: 0.955345060893\n",
      "Test Set\n",
      "-ACC: 0.974666666667\n",
      "-REC: 0.957300275482\n",
      "-F-Score: 0.948158253752\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\64bit\\envs\\py3.6\\lib\\site-packages\\sklearn\\utils\\validation.py:444: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder,OneHotEncoder\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import NearestNeighbors,KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB,BernoulliNB\n",
    "from sklearn.tree import DecisionTreeClassifier,export_graphviz\n",
    "from sklearn.externals.six import StringIO  # 画决策树可能会用到\n",
    "\n",
    "from sklearn.metrics import accuracy_score,recall_score,f1_score\n",
    "\n",
    "import os\n",
    "os.environ['PATH']+=os.pathsep+'D:/graphviz/bin/'\n",
    "import pydotplus\n",
    "# sl:satisfaction_level——False:MinMaxScaler;True:StandardScaler\n",
    "# le:last_evaluation——False:MinMaxScaler;True:StandardScaler\n",
    "# npr:number_project——False:MinMaxScaler;True:StandardScaler\n",
    "# amh:average_monthly_hours——False:MinMaxScaler;True:StandardScaler\n",
    "# tsc:time_spend_company——False:MinMaxScaler;True:StandardScaler\n",
    "# wa:Work_accident——False:MinMaxScaler;True:StandardScaler\n",
    "# pl5:promotion_last_5years——False:MinMaxScaler;True:StandardScaler\n",
    "# dp:deparment——False:LabelEncoding;True:OneHotEncoding\n",
    "# slr:salary——False:LabelEncoding;True:OneHotEncoding\n",
    "# lower_d——False:NotlowerDimension\n",
    "# ld_n——to n dimensions\n",
    "def hr_preprocessing(sl=False,le=False,npr=False,amh=False,tsc=False,wa=False,pl5=False,dp=True,slr=False,\n",
    "                     lower_d=False,ld_n=1):\n",
    "    df=pd.read_csv('./data/HR.csv')\n",
    "    # 1、清洗数据\n",
    "    df=df.dropna(subset=['satisfaction_level','last_evaluation'])\n",
    "    df=df[df['satisfaction_level']<=1][df['salary']!='nme']\n",
    "    # 2、得到标注\n",
    "    label=df['left']\n",
    "    df=df.drop('left',axis=1)\n",
    "    # 3、特征选择(例子中的特征先全部保留)\n",
    "    # 4、特征处理\n",
    "    scaler_lst=[sl,le,npr,amh,tsc,wa,pl5]\n",
    "    column_lst=['satisfaction_level','last_evaluation','number_project','average_monthly_hours',\n",
    "                'time_spend_company','Work_accident','promotion_last_5years']\n",
    "    for i in range(len(scaler_lst)):\n",
    "        if not scaler_lst[i]:\n",
    "            df[column_lst[i]]=MinMaxScaler().fit_transform(df[column_lst[i]].values.reshape(-1,1)).reshape(1,-1)[0]\n",
    "        else:\n",
    "            df[column_lst[i]]=StandardScaler().fit_transform(df[column_lst[i]].values.reshape(-1,1)).reshape(1,-1)[0]\n",
    "    scaler_lst=[dp,slr]\n",
    "    column_lst=['department','salary']\n",
    "    for i in range(len(scaler_lst)):\n",
    "        if not scaler_lst[i]:\n",
    "            if column_lst[i]=='salary':\n",
    "                # 由于LabelEncoding会按照字母顺序来确定0,1,2，破坏了low,med,high的顺序，所以需要重新定义一个函数map_salary               \n",
    "                df[column_lst[i]]=[map_salary(s) for s in df[column_lst[i]].values]\n",
    "            else:\n",
    "                df[column_lst[i]]=LabelEncoder().fit_transform(df[column_lst[i]])  \n",
    "            # LabelEncoding之后，进行一下归一化处理\n",
    "            df[column_lst[i]]=MinMaxScaler().fit_transform(df[column_lst[i]].values.reshape(-1,1)).reshape(1,-1)[0]\n",
    "        else:\n",
    "            # OneHot编码，可以直接用pandas里面的get_dummies\n",
    "            df=pd.get_dummies(df,columns=[column_lst[i]])\n",
    "    if lower_d:\n",
    "        # PCA降维与标注Label无关，而LDA降维的n_components不能超过Label的类别（由于left只有0,1，故LDA只能降成1维）\n",
    "        return PCA(n_components=ld_n).fit_transform(df.values),label\n",
    "    return df,label\n",
    "\n",
    "def map_salary(s):\n",
    "    d=dict([('low',0),('medium',1),('high',2)])\n",
    "    return d.get(s,0)  # 将low,med,high分别赋值0,1,2，如果没有找到则赋值为0\n",
    "\n",
    "def hr_modeling(features,label):\n",
    "    f_v=features.values\n",
    "    f_names=fetures.columns.values\n",
    "    l_v=label.values\n",
    "    # 先把验证集分离出来，再分割训练集和测试集。训练集、验证集、测试集之比6:2:2。\n",
    "    X_tt,X_validation,Y_tt,Y_validation=train_test_split(f_v,l_v,test_size=0.2)\n",
    "    X_train,X_test,Y_train,Y_test=train_test_split(X_tt,Y_tt,test_size=0.25)\n",
    "\n",
    "    # KNN\n",
    "    models=[]\n",
    "    #models.append(('KNN',KNeighborsClassifier(n_neighbors=3)))\n",
    "    # 朴素贝叶斯\n",
    "    #models.append(('GaussianNB',GaussianNB()))\n",
    "    #models.append(('BernoulliNB',BernoulliNB()))\n",
    "    # 决策树\n",
    "    models.append(('DecisionTreeGini',DecisionTreeClassifier()))\n",
    "    models.append(('DecisionTreeEntropy',DecisionTreeClassifier(criterion='entropy')))\n",
    "    \n",
    "    for clf_name,clf in models:\n",
    "        clf.fit(X_train,Y_train)\n",
    "        xy_lst=[(X_train,Y_train),(X_validation,Y_validation),(X_test,Y_test)]\n",
    "        d=dict([(0,'Train Set'),(1,'Validation Set'),(2,'Test Set')])\n",
    "        print('-'*8,clf_name,'-'*8)\n",
    "        for i in range(len(xy_lst)):\n",
    "            X_part=xy_lst[i][0]\n",
    "            Y_part=xy_lst[i][1]\n",
    "            Y_pred=clf.predict(X_part)\n",
    "            print(d.get(i))\n",
    "            print('-ACC:',accuracy_score(Y_part,Y_pred))\n",
    "            print('-REC:',recall_score(Y_part,Y_pred))\n",
    "            print('-F-Score:',f1_score(Y_part,Y_pred))\n",
    "            \"\"\"\n",
    "            dot_data=export_graphviz(clf,out_file=None,\n",
    "                                     feature_names=f_names,\n",
    "                                     class_names=['NL','L'],\n",
    "                                     filled=True,\n",
    "                                     rounded=True,\n",
    "                                     special_characters=True)\n",
    "            graph=pydotplus.graph_from_dot_data(dot_data)\n",
    "            graph.write_pdf('dt_tree.pdf')\n",
    "            \"\"\"\n",
    "            \"\"\"\n",
    "            # 以上画决策树这一段用上StringIO可以这么写（与上段等价）：\n",
    "            dot_data=StringIO()\n",
    "            export_graphviz(clf,out_file=dot_data,\n",
    "                            feature_names=f_names,\n",
    "                            class_names=['NL','L'],\n",
    "                            filled=True,\n",
    "                            rounded=True,\n",
    "                            special_characters=True)\n",
    "            graph=pydotplus.graph_from_dot_data(dot_data.getvalue())\n",
    "            graph.write_pdf('dt_tree.pdf')\n",
    "            \"\"\"\n",
    "            \n",
    "\n",
    "\n",
    "fetures,label=hr_preprocessing(dp=False)\n",
    "hr_modeling(fetures,label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "尝试其他剪枝方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- DecisionTreeGini --------\n",
      "Train Set\n",
      "-ACC: 0.977775308368\n",
      "-REC: 0.930794240595\n",
      "-F-Score: 0.95247148289\n",
      "Validation Set\n",
      "-ACC: 0.972\n",
      "-REC: 0.916317991632\n",
      "-F-Score: 0.93991416309\n",
      "Test Set\n",
      "-ACC: 0.974\n",
      "-REC: 0.920114122682\n",
      "-F-Score: 0.94298245614\n",
      "-------- DecisionTreeEntropy --------\n",
      "Train Set\n",
      "-ACC: 1.0\n",
      "-REC: 1.0\n",
      "-F-Score: 1.0\n",
      "Validation Set\n",
      "-ACC: 0.975\n",
      "-REC: 0.958158995816\n",
      "-F-Score: 0.948240165631\n",
      "Test Set\n",
      "-ACC: 0.974666666667\n",
      "-REC: 0.971469329529\n",
      "-F-Score: 0.947148817803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\64bit\\envs\\py3.6\\lib\\site-packages\\sklearn\\utils\\validation.py:444: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "D:\\anaconda\\64bit\\envs\\py3.6\\lib\\site-packages\\sklearn\\tree\\tree.py:282: DeprecationWarning: The min_impurity_split parameter is deprecated and will be removed in version 0.21. Use the min_impurity_decrease parameter instead.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder,OneHotEncoder\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import NearestNeighbors,KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB,BernoulliNB\n",
    "from sklearn.tree import DecisionTreeClassifier,export_graphviz\n",
    "from sklearn.externals.six import StringIO  # 画决策树可能会用到\n",
    "\n",
    "from sklearn.metrics import accuracy_score,recall_score,f1_score\n",
    "\n",
    "import os\n",
    "os.environ['PATH']+=os.pathsep+'D:/graphviz/bin/'\n",
    "import pydotplus\n",
    "# sl:satisfaction_level——False:MinMaxScaler;True:StandardScaler\n",
    "# le:last_evaluation——False:MinMaxScaler;True:StandardScaler\n",
    "# npr:number_project——False:MinMaxScaler;True:StandardScaler\n",
    "# amh:average_monthly_hours——False:MinMaxScaler;True:StandardScaler\n",
    "# tsc:time_spend_company——False:MinMaxScaler;True:StandardScaler\n",
    "# wa:Work_accident——False:MinMaxScaler;True:StandardScaler\n",
    "# pl5:promotion_last_5years——False:MinMaxScaler;True:StandardScaler\n",
    "# dp:deparment——False:LabelEncoding;True:OneHotEncoding\n",
    "# slr:salary——False:LabelEncoding;True:OneHotEncoding\n",
    "# lower_d——False:NotlowerDimension\n",
    "# ld_n——to n dimensions\n",
    "def hr_preprocessing(sl=False,le=False,npr=False,amh=False,tsc=False,wa=False,pl5=False,dp=True,slr=False,\n",
    "                     lower_d=False,ld_n=1):\n",
    "    df=pd.read_csv('./data/HR.csv')\n",
    "    # 1、清洗数据\n",
    "    df=df.dropna(subset=['satisfaction_level','last_evaluation'])\n",
    "    df=df[df['satisfaction_level']<=1][df['salary']!='nme']\n",
    "    # 2、得到标注\n",
    "    label=df['left']\n",
    "    df=df.drop('left',axis=1)\n",
    "    # 3、特征选择(例子中的特征先全部保留)\n",
    "    # 4、特征处理\n",
    "    scaler_lst=[sl,le,npr,amh,tsc,wa,pl5]\n",
    "    column_lst=['satisfaction_level','last_evaluation','number_project','average_monthly_hours',\n",
    "                'time_spend_company','Work_accident','promotion_last_5years']\n",
    "    for i in range(len(scaler_lst)):\n",
    "        if not scaler_lst[i]:\n",
    "            df[column_lst[i]]=MinMaxScaler().fit_transform(df[column_lst[i]].values.reshape(-1,1)).reshape(1,-1)[0]\n",
    "        else:\n",
    "            df[column_lst[i]]=StandardScaler().fit_transform(df[column_lst[i]].values.reshape(-1,1)).reshape(1,-1)[0]\n",
    "    scaler_lst=[dp,slr]\n",
    "    column_lst=['department','salary']\n",
    "    for i in range(len(scaler_lst)):\n",
    "        if not scaler_lst[i]:\n",
    "            if column_lst[i]=='salary':\n",
    "                # 由于LabelEncoding会按照字母顺序来确定0,1,2，破坏了low,med,high的顺序，所以需要重新定义一个函数map_salary               \n",
    "                df[column_lst[i]]=[map_salary(s) for s in df[column_lst[i]].values]\n",
    "            else:\n",
    "                df[column_lst[i]]=LabelEncoder().fit_transform(df[column_lst[i]])  \n",
    "            # LabelEncoding之后，进行一下归一化处理\n",
    "            df[column_lst[i]]=MinMaxScaler().fit_transform(df[column_lst[i]].values.reshape(-1,1)).reshape(1,-1)[0]\n",
    "        else:\n",
    "            # OneHot编码，可以直接用pandas里面的get_dummies\n",
    "            df=pd.get_dummies(df,columns=[column_lst[i]])\n",
    "    if lower_d:\n",
    "        # PCA降维与标注Label无关，而LDA降维的n_components不能超过Label的类别（由于left只有0,1，故LDA只能降成1维）\n",
    "        return PCA(n_components=ld_n).fit_transform(df.values),label\n",
    "    return df,label\n",
    "\n",
    "def map_salary(s):\n",
    "    d=dict([('low',0),('medium',1),('high',2)])\n",
    "    return d.get(s,0)  # 将low,med,high分别赋值0,1,2，如果没有找到则赋值为0\n",
    "\n",
    "def hr_modeling(features,label):\n",
    "    f_v=features.values\n",
    "    f_names=fetures.columns.values\n",
    "    l_v=label.values\n",
    "    # 先把验证集分离出来，再分割训练集和测试集。训练集、验证集、测试集之比6:2:2。\n",
    "    X_tt,X_validation,Y_tt,Y_validation=train_test_split(f_v,l_v,test_size=0.2)\n",
    "    X_train,X_test,Y_train,Y_test=train_test_split(X_tt,Y_tt,test_size=0.25)\n",
    "\n",
    "    # KNN\n",
    "    models=[]\n",
    "    #models.append(('KNN',KNeighborsClassifier(n_neighbors=3)))\n",
    "    # 朴素贝叶斯\n",
    "    #models.append(('GaussianNB',GaussianNB()))\n",
    "    #models.append(('BernoulliNB',BernoulliNB()))\n",
    "    # 决策树\n",
    "    models.append(('DecisionTreeGini',DecisionTreeClassifier(min_impurity_split=0.1)))\n",
    "    models.append(('DecisionTreeEntropy',DecisionTreeClassifier(criterion='entropy')))\n",
    "    \n",
    "    for clf_name,clf in models:\n",
    "        clf.fit(X_train,Y_train)\n",
    "        xy_lst=[(X_train,Y_train),(X_validation,Y_validation),(X_test,Y_test)]\n",
    "        d=dict([(0,'Train Set'),(1,'Validation Set'),(2,'Test Set')])\n",
    "        print('-'*8,clf_name,'-'*8)\n",
    "        for i in range(len(xy_lst)):\n",
    "            X_part=xy_lst[i][0]\n",
    "            Y_part=xy_lst[i][1]\n",
    "            Y_pred=clf.predict(X_part)\n",
    "            print(d.get(i))\n",
    "            print('-ACC:',accuracy_score(Y_part,Y_pred))\n",
    "            print('-REC:',recall_score(Y_part,Y_pred))\n",
    "            print('-F-Score:',f1_score(Y_part,Y_pred))\n",
    "            \"\"\"\n",
    "            dot_data=export_graphviz(clf,out_file=None,\n",
    "                                     feature_names=f_names,\n",
    "                                     class_names=['NL','L'],\n",
    "                                     filled=True,\n",
    "                                     rounded=True,\n",
    "                                     special_characters=True)\n",
    "            graph=pydotplus.graph_from_dot_data(dot_data)\n",
    "            graph.write_pdf('dt_tree.pdf')\n",
    "            \"\"\"\n",
    "            \"\"\"\n",
    "            # 以上画决策树这一段用上StringIO可以这么写（与上段等价）：\n",
    "            dot_data=StringIO()\n",
    "            export_graphviz(clf,out_file=dot_data,\n",
    "                            feature_names=f_names,\n",
    "                            class_names=['NL','L'],\n",
    "                            filled=True,\n",
    "                            rounded=True,\n",
    "                            special_characters=True)\n",
    "            graph=pydotplus.graph_from_dot_data(dot_data.getvalue())\n",
    "            graph.write_pdf('dt_tree.pdf')\n",
    "            \"\"\"\n",
    "            \n",
    "\n",
    "fetures,label=hr_preprocessing(dp=False)\n",
    "hr_modeling(fetures,label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "效果变差了，因此 min_impurity_split=0.1 这一条还是去掉比较好。经过各种尝试，可以调出适宜的模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分类——支持向量机（SVM）\n",
    "<img src='./image/6.6_1.png' width=500 />\n",
    "<img src='./image/6.6_2.png' width=500 />\n",
    "其中，每个$a_n$>0。\n",
    "<br />但是很多时候超平面无法直接将不同类的样本分开，这时候可以考虑扩维，如图：\n",
    "<img src='./image/6.6_3.png' width=500 />\n",
    "但是这样可能会导致维度灾难（维度过多），所以应该考虑先在地位计算再扩维。\n",
    "<img src='./image/6.6_4.png' width=500 />\n",
    "常用核函数：\n",
    "<img src='./image/6.6_5.png' width=500 />\n",
    "与决策树相比，SVM的边界更加平滑。\n",
    "<img src='./image/6.6_6.png' width=500 />\n",
    "<br />添加松弛变量：为了达到更宽的分界线，有时需要容忍少量的错分点，减少过拟合现象。\n",
    "<br /><img src='./image/6.6_7.png' width=500 />\n",
    "——》\n",
    "<img src='./image/6.6_8.png' width=500 div />  \n",
    "<br />根据问题实际场景，对不同的标注赋予不同的权值，得到更加合理的边界。（图中黑色线可能比红色更合理）\n",
    "<br /><img src='./image/6.6_9.png' width=500 />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\64bit\\envs\\py3.6\\lib\\site-packages\\sklearn\\utils\\validation.py:444: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- SVM Classifier --------\n",
      "Train Set\n",
      "-ACC: 0.912101344594\n",
      "-REC: 0.740723774622\n",
      "-F-Score: 0.80347826087\n",
      "Validation Set\n",
      "-ACC: 0.913\n",
      "-REC: 0.744588744589\n",
      "-F-Score: 0.798143851508\n",
      "Test Set\n",
      "-ACC: 0.917\n",
      "-REC: 0.746762589928\n",
      "-F-Score: 0.806526806527\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder,OneHotEncoder\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import NearestNeighbors,KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB,BernoulliNB\n",
    "from sklearn.tree import DecisionTreeClassifier,export_graphviz\n",
    "from sklearn.externals.six import StringIO  # 画决策树可能会用到\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.metrics import accuracy_score,recall_score,f1_score\n",
    "\n",
    "import os\n",
    "os.environ['PATH']+=os.pathsep+'D:/graphviz/bin/'\n",
    "import pydotplus\n",
    "# sl:satisfaction_level——False:MinMaxScaler;True:StandardScaler\n",
    "# le:last_evaluation——False:MinMaxScaler;True:StandardScaler\n",
    "# npr:number_project——False:MinMaxScaler;True:StandardScaler\n",
    "# amh:average_monthly_hours——False:MinMaxScaler;True:StandardScaler\n",
    "# tsc:time_spend_company——False:MinMaxScaler;True:StandardScaler\n",
    "# wa:Work_accident——False:MinMaxScaler;True:StandardScaler\n",
    "# pl5:promotion_last_5years——False:MinMaxScaler;True:StandardScaler\n",
    "# dp:deparment——False:LabelEncoding;True:OneHotEncoding\n",
    "# slr:salary——False:LabelEncoding;True:OneHotEncoding\n",
    "# lower_d——False:NotlowerDimension\n",
    "# ld_n——to n dimensions\n",
    "def hr_preprocessing(sl=False,le=False,npr=False,amh=False,tsc=False,wa=False,pl5=False,dp=True,slr=False,\n",
    "                     lower_d=False,ld_n=1):\n",
    "    df=pd.read_csv('./data/HR.csv')\n",
    "    # 1、清洗数据\n",
    "    df=df.dropna(subset=['satisfaction_level','last_evaluation'])\n",
    "    df=df[df['satisfaction_level']<=1][df['salary']!='nme']\n",
    "    # 2、得到标注\n",
    "    label=df['left']\n",
    "    df=df.drop('left',axis=1)\n",
    "    # 3、特征选择(例子中的特征先全部保留)\n",
    "    # 4、特征处理\n",
    "    scaler_lst=[sl,le,npr,amh,tsc,wa,pl5]\n",
    "    column_lst=['satisfaction_level','last_evaluation','number_project','average_monthly_hours',\n",
    "                'time_spend_company','Work_accident','promotion_last_5years']\n",
    "    for i in range(len(scaler_lst)):\n",
    "        if not scaler_lst[i]:\n",
    "            df[column_lst[i]]=MinMaxScaler().fit_transform(df[column_lst[i]].values.reshape(-1,1)).reshape(1,-1)[0]\n",
    "        else:\n",
    "            df[column_lst[i]]=StandardScaler().fit_transform(df[column_lst[i]].values.reshape(-1,1)).reshape(1,-1)[0]\n",
    "    scaler_lst=[dp,slr]\n",
    "    column_lst=['department','salary']\n",
    "    for i in range(len(scaler_lst)):\n",
    "        if not scaler_lst[i]:\n",
    "            if column_lst[i]=='salary':\n",
    "                # 由于LabelEncoding会按照字母顺序来确定0,1,2，破坏了low,med,high的顺序，所以需要重新定义一个函数map_salary               \n",
    "                df[column_lst[i]]=[map_salary(s) for s in df[column_lst[i]].values]\n",
    "            else:\n",
    "                df[column_lst[i]]=LabelEncoder().fit_transform(df[column_lst[i]])  \n",
    "            # LabelEncoding之后，进行一下归一化处理\n",
    "            df[column_lst[i]]=MinMaxScaler().fit_transform(df[column_lst[i]].values.reshape(-1,1)).reshape(1,-1)[0]\n",
    "        else:\n",
    "            # OneHot编码，可以直接用pandas里面的get_dummies\n",
    "            df=pd.get_dummies(df,columns=[column_lst[i]])\n",
    "    if lower_d:\n",
    "        # PCA降维与标注Label无关，而LDA降维的n_components不能超过Label的类别（由于left只有0,1，故LDA只能降成1维）\n",
    "        return PCA(n_components=ld_n).fit_transform(df.values),label\n",
    "    return df,label\n",
    "\n",
    "def map_salary(s):\n",
    "    d=dict([('low',0),('medium',1),('high',2)])\n",
    "    return d.get(s,0)  # 将low,med,high分别赋值0,1,2，如果没有找到则赋值为0\n",
    "\n",
    "def hr_modeling(features,label):\n",
    "    f_v=features.values\n",
    "    f_names=fetures.columns.values\n",
    "    l_v=label.values\n",
    "    # 先把验证集分离出来，再分割训练集和测试集。训练集、验证集、测试集之比6:2:2。\n",
    "    X_tt,X_validation,Y_tt,Y_validation=train_test_split(f_v,l_v,test_size=0.2)\n",
    "    X_train,X_test,Y_train,Y_test=train_test_split(X_tt,Y_tt,test_size=0.25)\n",
    "\n",
    "    models=[]\n",
    "#     # KNN\n",
    "#     models.append(('KNN',KNeighborsClassifier(n_neighbors=3)))\n",
    "#     # 朴素贝叶斯\n",
    "#     models.append(('GaussianNB',GaussianNB()))\n",
    "#     models.append(('BernoulliNB',BernoulliNB()))\n",
    "#     # 决策树\n",
    "#     models.append(('DecisionTreeGini',DecisionTreeClassifier()))\n",
    "#     models.append(('DecisionTreeEntropy',DecisionTreeClassifier(criterion='entropy')))\n",
    "    # SVM\n",
    "    models.append(('SVM Classifier',SVC()))\n",
    "    \n",
    "    for clf_name,clf in models:\n",
    "        clf.fit(X_train,Y_train)\n",
    "        xy_lst=[(X_train,Y_train),(X_validation,Y_validation),(X_test,Y_test)]\n",
    "        d=dict([(0,'Train Set'),(1,'Validation Set'),(2,'Test Set')])\n",
    "        print('-'*8,clf_name,'-'*8)\n",
    "        for i in range(len(xy_lst)):\n",
    "            X_part=xy_lst[i][0]\n",
    "            Y_part=xy_lst[i][1]\n",
    "            Y_pred=clf.predict(X_part)\n",
    "            print(d.get(i))\n",
    "            print('-ACC:',accuracy_score(Y_part,Y_pred))\n",
    "            print('-REC:',recall_score(Y_part,Y_pred))\n",
    "            print('-F-Score:',f1_score(Y_part,Y_pred))\n",
    "            \"\"\"\n",
    "            dot_data=export_graphviz(clf,out_file=None,\n",
    "                                     feature_names=f_names,\n",
    "                                     class_names=['NL','L'],\n",
    "                                     filled=True,\n",
    "                                     rounded=True,\n",
    "                                     special_characters=True)\n",
    "            graph=pydotplus.graph_from_dot_data(dot_data)\n",
    "            graph.write_pdf('dt_tree.pdf')\n",
    "            \"\"\"\n",
    "            \"\"\"\n",
    "            # 以上画决策树这一段用上StringIO可以这么写（与上段等价）：\n",
    "            dot_data=StringIO()\n",
    "            export_graphviz(clf,out_file=dot_data,\n",
    "                            feature_names=f_names,\n",
    "                            class_names=['NL','L'],\n",
    "                            filled=True,\n",
    "                            rounded=True,\n",
    "                            special_characters=True)\n",
    "            graph=pydotplus.graph_from_dot_data(dot_data.getvalue())\n",
    "            graph.write_pdf('dt_tree.pdf')\n",
    "            \"\"\"\n",
    "            \n",
    "\n",
    "fetures,label=hr_preprocessing(dp=False)\n",
    "hr_modeling(fetures,label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "效果不算很好，需要试着调节一下SVM的参数。比如C=100000，C表示分错类别的惩罚力度，默认为1。如果惩罚加大，错分点就会减少，代价是电脑运行速度会减慢。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\64bit\\envs\\py3.6\\lib\\site-packages\\sklearn\\utils\\validation.py:444: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- SVM Classifier --------\n",
      "Train Set\n",
      "-ACC: 0.970218913213\n",
      "-REC: 0.925501432665\n",
      "-F-Score: 0.935328185328\n",
      "Validation Set\n",
      "-ACC: 0.961333333333\n",
      "-REC: 0.909574468085\n",
      "-F-Score: 0.921832884097\n",
      "Test Set\n",
      "-ACC: 0.958\n",
      "-REC: 0.906206896552\n",
      "-F-Score: 0.9125\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder,OneHotEncoder\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import NearestNeighbors,KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB,BernoulliNB\n",
    "from sklearn.tree import DecisionTreeClassifier,export_graphviz\n",
    "from sklearn.externals.six import StringIO  # 画决策树可能会用到\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.metrics import accuracy_score,recall_score,f1_score\n",
    "\n",
    "import os\n",
    "os.environ['PATH']+=os.pathsep+'D:/graphviz/bin/'\n",
    "import pydotplus\n",
    "# sl:satisfaction_level——False:MinMaxScaler;True:StandardScaler\n",
    "# le:last_evaluation——False:MinMaxScaler;True:StandardScaler\n",
    "# npr:number_project——False:MinMaxScaler;True:StandardScaler\n",
    "# amh:average_monthly_hours——False:MinMaxScaler;True:StandardScaler\n",
    "# tsc:time_spend_company——False:MinMaxScaler;True:StandardScaler\n",
    "# wa:Work_accident——False:MinMaxScaler;True:StandardScaler\n",
    "# pl5:promotion_last_5years——False:MinMaxScaler;True:StandardScaler\n",
    "# dp:deparment——False:LabelEncoding;True:OneHotEncoding\n",
    "# slr:salary——False:LabelEncoding;True:OneHotEncoding\n",
    "# lower_d——False:NotlowerDimension\n",
    "# ld_n——to n dimensions\n",
    "def hr_preprocessing(sl=False,le=False,npr=False,amh=False,tsc=False,wa=False,pl5=False,dp=True,slr=False,\n",
    "                     lower_d=False,ld_n=1):\n",
    "    df=pd.read_csv('./data/HR.csv')\n",
    "    # 1、清洗数据\n",
    "    df=df.dropna(subset=['satisfaction_level','last_evaluation'])\n",
    "    df=df[df['satisfaction_level']<=1][df['salary']!='nme']\n",
    "    # 2、得到标注\n",
    "    label=df['left']\n",
    "    df=df.drop('left',axis=1)\n",
    "    # 3、特征选择(例子中的特征先全部保留)\n",
    "    # 4、特征处理\n",
    "    scaler_lst=[sl,le,npr,amh,tsc,wa,pl5]\n",
    "    column_lst=['satisfaction_level','last_evaluation','number_project','average_monthly_hours',\n",
    "                'time_spend_company','Work_accident','promotion_last_5years']\n",
    "    for i in range(len(scaler_lst)):\n",
    "        if not scaler_lst[i]:\n",
    "            df[column_lst[i]]=MinMaxScaler().fit_transform(df[column_lst[i]].values.reshape(-1,1)).reshape(1,-1)[0]\n",
    "        else:\n",
    "            df[column_lst[i]]=StandardScaler().fit_transform(df[column_lst[i]].values.reshape(-1,1)).reshape(1,-1)[0]\n",
    "    scaler_lst=[dp,slr]\n",
    "    column_lst=['department','salary']\n",
    "    for i in range(len(scaler_lst)):\n",
    "        if not scaler_lst[i]:\n",
    "            if column_lst[i]=='salary':\n",
    "                # 由于LabelEncoding会按照字母顺序来确定0,1,2，破坏了low,med,high的顺序，所以需要重新定义一个函数map_salary               \n",
    "                df[column_lst[i]]=[map_salary(s) for s in df[column_lst[i]].values]\n",
    "            else:\n",
    "                df[column_lst[i]]=LabelEncoder().fit_transform(df[column_lst[i]])  \n",
    "            # LabelEncoding之后，进行一下归一化处理\n",
    "            df[column_lst[i]]=MinMaxScaler().fit_transform(df[column_lst[i]].values.reshape(-1,1)).reshape(1,-1)[0]\n",
    "        else:\n",
    "            # OneHot编码，可以直接用pandas里面的get_dummies\n",
    "            df=pd.get_dummies(df,columns=[column_lst[i]])\n",
    "    if lower_d:\n",
    "        # PCA降维与标注Label无关，而LDA降维的n_components不能超过Label的类别（由于left只有0,1，故LDA只能降成1维）\n",
    "        return PCA(n_components=ld_n).fit_transform(df.values),label\n",
    "    return df,label\n",
    "\n",
    "def map_salary(s):\n",
    "    d=dict([('low',0),('medium',1),('high',2)])\n",
    "    return d.get(s,0)  # 将low,med,high分别赋值0,1,2，如果没有找到则赋值为0\n",
    "\n",
    "def hr_modeling(features,label):\n",
    "    f_v=features.values\n",
    "    f_names=fetures.columns.values\n",
    "    l_v=label.values\n",
    "    # 先把验证集分离出来，再分割训练集和测试集。训练集、验证集、测试集之比6:2:2。\n",
    "    X_tt,X_validation,Y_tt,Y_validation=train_test_split(f_v,l_v,test_size=0.2)\n",
    "    X_train,X_test,Y_train,Y_test=train_test_split(X_tt,Y_tt,test_size=0.25)\n",
    "\n",
    "    models=[]\n",
    "#     # KNN\n",
    "#     models.append(('KNN',KNeighborsClassifier(n_neighbors=3)))\n",
    "#     # 朴素贝叶斯\n",
    "#     models.append(('GaussianNB',GaussianNB()))\n",
    "#     models.append(('BernoulliNB',BernoulliNB()))\n",
    "#     # 决策树\n",
    "#     models.append(('DecisionTreeGini',DecisionTreeClassifier()))\n",
    "#     models.append(('DecisionTreeEntropy',DecisionTreeClassifier(criterion='entropy')))\n",
    "#     # SVM\n",
    "#     models.append(('SVM Classifier',SVC(C=100)))\n",
    "    # 分类——集成——随机森林\n",
    "    \n",
    "    \n",
    "    for clf_name,clf in models:\n",
    "        clf.fit(X_train,Y_train)\n",
    "        xy_lst=[(X_train,Y_train),(X_validation,Y_validation),(X_test,Y_test)]\n",
    "        d=dict([(0,'Train Set'),(1,'Validation Set'),(2,'Test Set')])\n",
    "        print('-'*8,clf_name,'-'*8)\n",
    "        for i in range(len(xy_lst)):\n",
    "            X_part=xy_lst[i][0]\n",
    "            Y_part=xy_lst[i][1]\n",
    "            Y_pred=clf.predict(X_part)\n",
    "            print(d.get(i))\n",
    "            print('-ACC:',accuracy_score(Y_part,Y_pred))\n",
    "            print('-REC:',recall_score(Y_part,Y_pred))\n",
    "            print('-F-Score:',f1_score(Y_part,Y_pred))\n",
    "            \"\"\"\n",
    "            dot_data=export_graphviz(clf,out_file=None,\n",
    "                                     feature_names=f_names,\n",
    "                                     class_names=['NL','L'],\n",
    "                                     filled=True,\n",
    "                                     rounded=True,\n",
    "                                     special_characters=True)\n",
    "            graph=pydotplus.graph_from_dot_data(dot_data)\n",
    "            graph.write_pdf('dt_tree.pdf')\n",
    "            \"\"\"\n",
    "            \"\"\"\n",
    "            # 以上画决策树这一段用上StringIO可以这么写（与上段等价）：\n",
    "            dot_data=StringIO()\n",
    "            export_graphviz(clf,out_file=dot_data,\n",
    "                            feature_names=f_names,\n",
    "                            class_names=['NL','L'],\n",
    "                            filled=True,\n",
    "                            rounded=True,\n",
    "                            special_characters=True)\n",
    "            graph=pydotplus.graph_from_dot_data(dot_data.getvalue())\n",
    "            graph.write_pdf('dt_tree.pdf')\n",
    "            \"\"\"\n",
    "            \n",
    "\n",
    "fetures,label=hr_preprocessing(dp=False)\n",
    "hr_modeling(fetures,label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分类——集成——随机森林\n",
    "<img src='./image/6.7_2.png' width=500 />\n",
    "<img src='./image/6.7_3.png' width=500 />\n",
    "<img src='./image/6.7.png' width=500 />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- DecisionTreeGini --------\n",
      "Train Set\n",
      "-ACC: 1.0\n",
      "-REC: 1.0\n",
      "-F-Score: 1.0\n",
      "Validation Set\n",
      "-ACC: 0.974666666667\n",
      "-REC: 0.962447844228\n",
      "-F-Score: 0.947945205479\n",
      "Test Set\n",
      "-ACC: 0.973\n",
      "-REC: 0.960164835165\n",
      "-F-Score: 0.94523326572\n",
      "-------- DecisionTreeEntropy --------\n",
      "Train Set\n",
      "-ACC: 1.0\n",
      "-REC: 1.0\n",
      "-F-Score: 1.0\n",
      "Validation Set\n",
      "-ACC: 0.972\n",
      "-REC: 0.956884561892\n",
      "-F-Score: 0.942465753425\n",
      "Test Set\n",
      "-ACC: 0.981\n",
      "-REC: 0.957417582418\n",
      "-F-Score: 0.960716747071\n",
      "-------- RandomForest --------\n",
      "Train Set\n",
      "-ACC: 0.997777530837\n",
      "-REC: 0.991054613936\n",
      "-F-Score: 0.995271867612\n",
      "Validation Set\n",
      "-ACC: 0.987333333333\n",
      "-REC: 0.95966620306\n",
      "-F-Score: 0.973201692525\n",
      "Test Set\n",
      "-ACC: 0.985333333333\n",
      "-REC: 0.949175824176\n",
      "-F-Score: 0.969144460028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\64bit\\envs\\py3.6\\lib\\site-packages\\sklearn\\utils\\validation.py:444: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder,OneHotEncoder\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import NearestNeighbors,KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB,BernoulliNB\n",
    "from sklearn.tree import DecisionTreeClassifier,export_graphviz\n",
    "from sklearn.externals.six import StringIO  # 画决策树可能会用到\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score,recall_score,f1_score\n",
    "\n",
    "import os\n",
    "os.environ['PATH']+=os.pathsep+'D:/graphviz/bin/'\n",
    "import pydotplus\n",
    "# sl:satisfaction_level——False:MinMaxScaler;True:StandardScaler\n",
    "# le:last_evaluation——False:MinMaxScaler;True:StandardScaler\n",
    "# npr:number_project——False:MinMaxScaler;True:StandardScaler\n",
    "# amh:average_monthly_hours——False:MinMaxScaler;True:StandardScaler\n",
    "# tsc:time_spend_company——False:MinMaxScaler;True:StandardScaler\n",
    "# wa:Work_accident——False:MinMaxScaler;True:StandardScaler\n",
    "# pl5:promotion_last_5years——False:MinMaxScaler;True:StandardScaler\n",
    "# dp:deparment——False:LabelEncoding;True:OneHotEncoding\n",
    "# slr:salary——False:LabelEncoding;True:OneHotEncoding\n",
    "# lower_d——False:NotlowerDimension\n",
    "# ld_n——to n dimensions\n",
    "def hr_preprocessing(sl=False,le=False,npr=False,amh=False,tsc=False,wa=False,pl5=False,dp=True,slr=False,\n",
    "                     lower_d=False,ld_n=1):\n",
    "    df=pd.read_csv('./data/HR.csv')\n",
    "    # 1、清洗数据\n",
    "    df=df.dropna(subset=['satisfaction_level','last_evaluation'])\n",
    "    df=df[df['satisfaction_level']<=1][df['salary']!='nme']\n",
    "    # 2、得到标注\n",
    "    label=df['left']\n",
    "    df=df.drop('left',axis=1)\n",
    "    # 3、特征选择(例子中的特征先全部保留)\n",
    "    # 4、特征处理\n",
    "    scaler_lst=[sl,le,npr,amh,tsc,wa,pl5]\n",
    "    column_lst=['satisfaction_level','last_evaluation','number_project','average_monthly_hours',\n",
    "                'time_spend_company','Work_accident','promotion_last_5years']\n",
    "    for i in range(len(scaler_lst)):\n",
    "        if not scaler_lst[i]:\n",
    "            df[column_lst[i]]=MinMaxScaler().fit_transform(df[column_lst[i]].values.reshape(-1,1)).reshape(1,-1)[0]\n",
    "        else:\n",
    "            df[column_lst[i]]=StandardScaler().fit_transform(df[column_lst[i]].values.reshape(-1,1)).reshape(1,-1)[0]\n",
    "    scaler_lst=[dp,slr]\n",
    "    column_lst=['department','salary']\n",
    "    for i in range(len(scaler_lst)):\n",
    "        if not scaler_lst[i]:\n",
    "            if column_lst[i]=='salary':\n",
    "                # 由于LabelEncoding会按照字母顺序来确定0,1,2，破坏了low,med,high的顺序，所以需要重新定义一个函数map_salary               \n",
    "                df[column_lst[i]]=[map_salary(s) for s in df[column_lst[i]].values]\n",
    "            else:\n",
    "                df[column_lst[i]]=LabelEncoder().fit_transform(df[column_lst[i]])  \n",
    "            # LabelEncoding之后，进行一下归一化处理\n",
    "            df[column_lst[i]]=MinMaxScaler().fit_transform(df[column_lst[i]].values.reshape(-1,1)).reshape(1,-1)[0]\n",
    "        else:\n",
    "            # OneHot编码，可以直接用pandas里面的get_dummies\n",
    "            df=pd.get_dummies(df,columns=[column_lst[i]])\n",
    "    if lower_d:\n",
    "        # PCA降维与标注Label无关，而LDA降维的n_components不能超过Label的类别（由于left只有0,1，故LDA只能降成1维）\n",
    "        return PCA(n_components=ld_n).fit_transform(df.values),label\n",
    "    return df,label\n",
    "\n",
    "def map_salary(s):\n",
    "    d=dict([('low',0),('medium',1),('high',2)])\n",
    "    return d.get(s,0)  # 将low,med,high分别赋值0,1,2，如果没有找到则赋值为0\n",
    "\n",
    "def hr_modeling(features,label):\n",
    "    f_v=features.values\n",
    "    f_names=fetures.columns.values\n",
    "    l_v=label.values\n",
    "    # 先把验证集分离出来，再分割训练集和测试集。训练集、验证集、测试集之比6:2:2。\n",
    "    X_tt,X_validation,Y_tt,Y_validation=train_test_split(f_v,l_v,test_size=0.2)\n",
    "    X_train,X_test,Y_train,Y_test=train_test_split(X_tt,Y_tt,test_size=0.25)\n",
    "\n",
    "    models=[]\n",
    "#     # KNN\n",
    "#     models.append(('KNN',KNeighborsClassifier(n_neighbors=3)))\n",
    "#     # 朴素贝叶斯\n",
    "#     models.append(('GaussianNB',GaussianNB()))\n",
    "#     models.append(('BernoulliNB',BernoulliNB()))\n",
    "    # 决策树\n",
    "    models.append(('DecisionTreeGini',DecisionTreeClassifier()))\n",
    "    models.append(('DecisionTreeEntropy',DecisionTreeClassifier(criterion='entropy')))\n",
    "#     # SVM\n",
    "#     models.append(('SVM Classifier',SVC(C=100)))\n",
    "    # 分类——集成——随机森林\n",
    "    models.append(('RandomForest',RandomForestClassifier()))\n",
    "    \n",
    "    for clf_name,clf in models:\n",
    "        clf.fit(X_train,Y_train)\n",
    "        xy_lst=[(X_train,Y_train),(X_validation,Y_validation),(X_test,Y_test)]\n",
    "        d=dict([(0,'Train Set'),(1,'Validation Set'),(2,'Test Set')])\n",
    "        print('-'*8,clf_name,'-'*8)\n",
    "        for i in range(len(xy_lst)):\n",
    "            X_part=xy_lst[i][0]\n",
    "            Y_part=xy_lst[i][1]\n",
    "            Y_pred=clf.predict(X_part)\n",
    "            print(d.get(i))\n",
    "            print('-ACC:',accuracy_score(Y_part,Y_pred))\n",
    "            print('-REC:',recall_score(Y_part,Y_pred))\n",
    "            print('-F-Score:',f1_score(Y_part,Y_pred))\n",
    "            \"\"\"\n",
    "            dot_data=export_graphviz(clf,out_file=None,\n",
    "                                     feature_names=f_names,\n",
    "                                     class_names=['NL','L'],\n",
    "                                     filled=True,\n",
    "                                     rounded=True,\n",
    "                                     special_characters=True)\n",
    "            graph=pydotplus.graph_from_dot_data(dot_data)\n",
    "            graph.write_pdf('dt_tree.pdf')\n",
    "            \"\"\"\n",
    "            \"\"\"\n",
    "            # 以上画决策树这一段用上StringIO可以这么写（与上段等价）：\n",
    "            dot_data=StringIO()\n",
    "            export_graphviz(clf,out_file=dot_data,\n",
    "                            feature_names=f_names,\n",
    "                            class_names=['NL','L'],\n",
    "                            filled=True,\n",
    "                            rounded=True,\n",
    "                            special_characters=True)\n",
    "            graph=pydotplus.graph_from_dot_data(dot_data.getvalue())\n",
    "            graph.write_pdf('dt_tree.pdf')\n",
    "            \"\"\"\n",
    "            \n",
    "\n",
    "fetures,label=hr_preprocessing(dp=False)\n",
    "hr_modeling(fetures,label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "发现随机森林的效果比决策树更好。这是集成带来的好处。接下来调整随机森林参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\64bit\\envs\\py3.6\\lib\\site-packages\\sklearn\\utils\\validation.py:444: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- OriginalRandomForest --------\n",
      "Train Set\n",
      "-ACC: 0.997888654295\n",
      "-REC: 0.991612301957\n",
      "-F-Score: 0.995555555556\n",
      "Validation Set\n",
      "-ACC: 0.986\n",
      "-REC: 0.951253481894\n",
      "-F-Score: 0.970170454545\n",
      "Test Set\n",
      "-ACC: 0.989666666667\n",
      "-REC: 0.958981612447\n",
      "-F-Score: 0.977649603461\n",
      "-------- RandomForest --------\n",
      "Train Set\n",
      "-ACC: 1.0\n",
      "-REC: 1.0\n",
      "-F-Score: 1.0\n",
      "Validation Set\n",
      "-ACC: 0.970333333333\n",
      "-REC: 0.956824512535\n",
      "-F-Score: 0.939166097061\n",
      "Test Set\n",
      "-ACC: 0.976666666667\n",
      "-REC: 0.968882602546\n",
      "-F-Score: 0.951388888889\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder,OneHotEncoder\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import NearestNeighbors,KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB,BernoulliNB\n",
    "from sklearn.tree import DecisionTreeClassifier,export_graphviz\n",
    "from sklearn.externals.six import StringIO  # 画决策树可能会用到\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score,recall_score,f1_score\n",
    "\n",
    "import os\n",
    "os.environ['PATH']+=os.pathsep+'D:/graphviz/bin/'\n",
    "import pydotplus\n",
    "# sl:satisfaction_level——False:MinMaxScaler;True:StandardScaler\n",
    "# le:last_evaluation——False:MinMaxScaler;True:StandardScaler\n",
    "# npr:number_project——False:MinMaxScaler;True:StandardScaler\n",
    "# amh:average_monthly_hours——False:MinMaxScaler;True:StandardScaler\n",
    "# tsc:time_spend_company——False:MinMaxScaler;True:StandardScaler\n",
    "# wa:Work_accident——False:MinMaxScaler;True:StandardScaler\n",
    "# pl5:promotion_last_5years——False:MinMaxScaler;True:StandardScaler\n",
    "# dp:deparment——False:LabelEncoding;True:OneHotEncoding\n",
    "# slr:salary——False:LabelEncoding;True:OneHotEncoding\n",
    "# lower_d——False:NotlowerDimension\n",
    "# ld_n——to n dimensions\n",
    "def hr_preprocessing(sl=False,le=False,npr=False,amh=False,tsc=False,wa=False,pl5=False,dp=True,slr=False,\n",
    "                     lower_d=False,ld_n=1):\n",
    "    df=pd.read_csv('./data/HR.csv')\n",
    "    # 1、清洗数据\n",
    "    df=df.dropna(subset=['satisfaction_level','last_evaluation'])\n",
    "    df=df[df['satisfaction_level']<=1][df['salary']!='nme']\n",
    "    # 2、得到标注\n",
    "    label=df['left']\n",
    "    df=df.drop('left',axis=1)\n",
    "    # 3、特征选择(例子中的特征先全部保留)\n",
    "    # 4、特征处理\n",
    "    scaler_lst=[sl,le,npr,amh,tsc,wa,pl5]\n",
    "    column_lst=['satisfaction_level','last_evaluation','number_project','average_monthly_hours',\n",
    "                'time_spend_company','Work_accident','promotion_last_5years']\n",
    "    for i in range(len(scaler_lst)):\n",
    "        if not scaler_lst[i]:\n",
    "            df[column_lst[i]]=MinMaxScaler().fit_transform(df[column_lst[i]].values.reshape(-1,1)).reshape(1,-1)[0]\n",
    "        else:\n",
    "            df[column_lst[i]]=StandardScaler().fit_transform(df[column_lst[i]].values.reshape(-1,1)).reshape(1,-1)[0]\n",
    "    scaler_lst=[dp,slr]\n",
    "    column_lst=['department','salary']\n",
    "    for i in range(len(scaler_lst)):\n",
    "        if not scaler_lst[i]:\n",
    "            if column_lst[i]=='salary':\n",
    "                # 由于LabelEncoding会按照字母顺序来确定0,1,2，破坏了low,med,high的顺序，所以需要重新定义一个函数map_salary               \n",
    "                df[column_lst[i]]=[map_salary(s) for s in df[column_lst[i]].values]\n",
    "            else:\n",
    "                df[column_lst[i]]=LabelEncoder().fit_transform(df[column_lst[i]])  \n",
    "            # LabelEncoding之后，进行一下归一化处理\n",
    "            df[column_lst[i]]=MinMaxScaler().fit_transform(df[column_lst[i]].values.reshape(-1,1)).reshape(1,-1)[0]\n",
    "        else:\n",
    "            # OneHot编码，可以直接用pandas里面的get_dummies\n",
    "            df=pd.get_dummies(df,columns=[column_lst[i]])\n",
    "    if lower_d:\n",
    "        # PCA降维与标注Label无关，而LDA降维的n_components不能超过Label的类别（由于left只有0,1，故LDA只能降成1维）\n",
    "        return PCA(n_components=ld_n).fit_transform(df.values),label\n",
    "    return df,label\n",
    "\n",
    "def map_salary(s):\n",
    "    d=dict([('low',0),('medium',1),('high',2)])\n",
    "    return d.get(s,0)  # 将low,med,high分别赋值0,1,2，如果没有找到则赋值为0\n",
    "\n",
    "def hr_modeling(features,label):\n",
    "    f_v=features.values\n",
    "    f_names=fetures.columns.values\n",
    "    l_v=label.values\n",
    "    # 先把验证集分离出来，再分割训练集和测试集。训练集、验证集、测试集之比6:2:2。\n",
    "    X_tt,X_validation,Y_tt,Y_validation=train_test_split(f_v,l_v,test_size=0.2)\n",
    "    X_train,X_test,Y_train,Y_test=train_test_split(X_tt,Y_tt,test_size=0.25)\n",
    "\n",
    "    models=[]\n",
    "#     # KNN\n",
    "#     models.append(('KNN',KNeighborsClassifier(n_neighbors=3)))\n",
    "#     # 朴素贝叶斯\n",
    "#     models.append(('GaussianNB',GaussianNB()))\n",
    "#     models.append(('BernoulliNB',BernoulliNB()))\n",
    "#     # 决策树\n",
    "#     models.append(('DecisionTreeGini',DecisionTreeClassifier()))\n",
    "#     models.append(('DecisionTreeEntropy',DecisionTreeClassifier(criterion='entropy')))\n",
    "#     # SVM\n",
    "#     models.append(('SVM Classifier',SVC(C=100)))\n",
    "    # 分类——集成——随机森林\n",
    "    models.append(('OriginalRandomForest',RandomForestClassifier()))\n",
    "    models.append(('RandomForest',RandomForestClassifier(n_estimators=81,max_features=None,bootstrap=False)))\n",
    "    \n",
    "    for clf_name,clf in models:\n",
    "        clf.fit(X_train,Y_train)\n",
    "        xy_lst=[(X_train,Y_train),(X_validation,Y_validation),(X_test,Y_test)]\n",
    "        d=dict([(0,'Train Set'),(1,'Validation Set'),(2,'Test Set')])\n",
    "        print('-'*8,clf_name,'-'*8)\n",
    "        for i in range(len(xy_lst)):\n",
    "            X_part=xy_lst[i][0]\n",
    "            Y_part=xy_lst[i][1]\n",
    "            Y_pred=clf.predict(X_part)\n",
    "            print(d.get(i))\n",
    "            print('-ACC:',accuracy_score(Y_part,Y_pred))\n",
    "            print('-REC:',recall_score(Y_part,Y_pred))\n",
    "            print('-F-Score:',f1_score(Y_part,Y_pred))\n",
    "            \"\"\"\n",
    "            dot_data=export_graphviz(clf,out_file=None,\n",
    "                                     feature_names=f_names,\n",
    "                                     class_names=['NL','L'],\n",
    "                                     filled=True,\n",
    "                                     rounded=True,\n",
    "                                     special_characters=True)\n",
    "            graph=pydotplus.graph_from_dot_data(dot_data)\n",
    "            graph.write_pdf('dt_tree.pdf')\n",
    "            \"\"\"\n",
    "            \"\"\"\n",
    "            # 以上画决策树这一段用上StringIO可以这么写（与上段等价）：\n",
    "            dot_data=StringIO()\n",
    "            export_graphviz(clf,out_file=dot_data,\n",
    "                            feature_names=f_names,\n",
    "                            class_names=['NL','L'],\n",
    "                            filled=True,\n",
    "                            rounded=True,\n",
    "                            special_characters=True)\n",
    "            graph=pydotplus.graph_from_dot_data(dot_data.getvalue())\n",
    "            graph.write_pdf('dt_tree.pdf')\n",
    "            \"\"\"\n",
    "            \n",
    "\n",
    "fetures,label=hr_preprocessing(dp=False)\n",
    "hr_modeling(fetures,label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分类——集成——Adaboost\n",
    "<img src='./image/6.8_1.png' width=500 />\n",
    "<img src='./image/6.8_2.png' width=700 />\n",
    "<img src='./image/6.8_3.png' width=700 />\n",
    "<img src='./image/6.8_4.png' width=500 />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\64bit\\envs\\py3.6\\lib\\site-packages\\sklearn\\utils\\validation.py:444: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Adaboost --------\n",
      "Train Set\n",
      "-ACC: 0.960551172352\n",
      "-REC: 0.911162790698\n",
      "-F-Score: 0.916920196583\n",
      "Validation Set\n",
      "-ACC: 0.966\n",
      "-REC: 0.916317991632\n",
      "-F-Score: 0.927966101695\n",
      "Test Set\n",
      "-ACC: 0.96\n",
      "-REC: 0.903409090909\n",
      "-F-Score: 0.913793103448\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder,OneHotEncoder\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import NearestNeighbors,KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB,BernoulliNB\n",
    "from sklearn.tree import DecisionTreeClassifier,export_graphviz\n",
    "from sklearn.externals.six import StringIO  # 画决策树可能会用到\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score,recall_score,f1_score\n",
    "\n",
    "import os\n",
    "os.environ['PATH']+=os.pathsep+'D:/graphviz/bin/'\n",
    "import pydotplus\n",
    "# sl:satisfaction_level——False:MinMaxScaler;True:StandardScaler\n",
    "# le:last_evaluation——False:MinMaxScaler;True:StandardScaler\n",
    "# npr:number_project——False:MinMaxScaler;True:StandardScaler\n",
    "# amh:average_monthly_hours——False:MinMaxScaler;True:StandardScaler\n",
    "# tsc:time_spend_company——False:MinMaxScaler;True:StandardScaler\n",
    "# wa:Work_accident——False:MinMaxScaler;True:StandardScaler\n",
    "# pl5:promotion_last_5years——False:MinMaxScaler;True:StandardScaler\n",
    "# dp:deparment——False:LabelEncoding;True:OneHotEncoding\n",
    "# slr:salary——False:LabelEncoding;True:OneHotEncoding\n",
    "# lower_d——False:NotlowerDimension\n",
    "# ld_n——to n dimensions\n",
    "def hr_preprocessing(sl=False,le=False,npr=False,amh=False,tsc=False,wa=False,pl5=False,dp=True,slr=False,\n",
    "                     lower_d=False,ld_n=1):\n",
    "    df=pd.read_csv('./data/HR.csv')\n",
    "    # 1、清洗数据\n",
    "    df=df.dropna(subset=['satisfaction_level','last_evaluation'])\n",
    "    df=df[df['satisfaction_level']<=1][df['salary']!='nme']\n",
    "    # 2、得到标注\n",
    "    label=df['left']\n",
    "    df=df.drop('left',axis=1)\n",
    "    # 3、特征选择(例子中的特征先全部保留)\n",
    "    # 4、特征处理\n",
    "    scaler_lst=[sl,le,npr,amh,tsc,wa,pl5]\n",
    "    column_lst=['satisfaction_level','last_evaluation','number_project','average_monthly_hours',\n",
    "                'time_spend_company','Work_accident','promotion_last_5years']\n",
    "    for i in range(len(scaler_lst)):\n",
    "        if not scaler_lst[i]:\n",
    "            df[column_lst[i]]=MinMaxScaler().fit_transform(df[column_lst[i]].values.reshape(-1,1)).reshape(1,-1)[0]\n",
    "        else:\n",
    "            df[column_lst[i]]=StandardScaler().fit_transform(df[column_lst[i]].values.reshape(-1,1)).reshape(1,-1)[0]\n",
    "    scaler_lst=[dp,slr]\n",
    "    column_lst=['department','salary']\n",
    "    for i in range(len(scaler_lst)):\n",
    "        if not scaler_lst[i]:\n",
    "            if column_lst[i]=='salary':\n",
    "                # 由于LabelEncoding会按照字母顺序来确定0,1,2，破坏了low,med,high的顺序，所以需要重新定义一个函数map_salary               \n",
    "                df[column_lst[i]]=[map_salary(s) for s in df[column_lst[i]].values]\n",
    "            else:\n",
    "                df[column_lst[i]]=LabelEncoder().fit_transform(df[column_lst[i]])  \n",
    "            # LabelEncoding之后，进行一下归一化处理\n",
    "            df[column_lst[i]]=MinMaxScaler().fit_transform(df[column_lst[i]].values.reshape(-1,1)).reshape(1,-1)[0]\n",
    "        else:\n",
    "            # OneHot编码，可以直接用pandas里面的get_dummies\n",
    "            df=pd.get_dummies(df,columns=[column_lst[i]])\n",
    "    if lower_d:\n",
    "        # PCA降维与标注Label无关，而LDA降维的n_components不能超过Label的类别（由于left只有0,1，故LDA只能降成1维）\n",
    "        return PCA(n_components=ld_n).fit_transform(df.values),label\n",
    "    return df,label\n",
    "\n",
    "def map_salary(s):\n",
    "    d=dict([('low',0),('medium',1),('high',2)])\n",
    "    return d.get(s,0)  # 将low,med,high分别赋值0,1,2，如果没有找到则赋值为0\n",
    "\n",
    "def hr_modeling(features,label):\n",
    "    f_v=features.values\n",
    "    f_names=fetures.columns.values\n",
    "    l_v=label.values\n",
    "    # 先把验证集分离出来，再分割训练集和测试集。训练集、验证集、测试集之比6:2:2。\n",
    "    X_tt,X_validation,Y_tt,Y_validation=train_test_split(f_v,l_v,test_size=0.2)\n",
    "    X_train,X_test,Y_train,Y_test=train_test_split(X_tt,Y_tt,test_size=0.25)\n",
    "\n",
    "    models=[]\n",
    "#     # KNN\n",
    "#     models.append(('KNN',KNeighborsClassifier(n_neighbors=3)))\n",
    "#     # 朴素贝叶斯\n",
    "#     models.append(('GaussianNB',GaussianNB()))\n",
    "#     models.append(('BernoulliNB',BernoulliNB()))\n",
    "#     # 决策树\n",
    "#     models.append(('DecisionTreeGini',DecisionTreeClassifier()))\n",
    "#     models.append(('DecisionTreeEntropy',DecisionTreeClassifier(criterion='entropy')))\n",
    "#     # SVM\n",
    "#     models.append(('SVM Classifier',SVC(C=100)))\n",
    "#     # 分类——集成——随机森林\n",
    "#     models.append(('RandomForest',RandomForestClassifier()))\n",
    "    # 分类——集成——Adaboost\n",
    "    models.append(('Adaboost',AdaBoostClassifier(n_estimators=100)))\n",
    "    \n",
    "    for clf_name,clf in models:\n",
    "        clf.fit(X_train,Y_train)\n",
    "        xy_lst=[(X_train,Y_train),(X_validation,Y_validation),(X_test,Y_test)]\n",
    "        d=dict([(0,'Train Set'),(1,'Validation Set'),(2,'Test Set')])\n",
    "        print('-'*8,clf_name,'-'*8)\n",
    "        for i in range(len(xy_lst)):\n",
    "            X_part=xy_lst[i][0]\n",
    "            Y_part=xy_lst[i][1]\n",
    "            Y_pred=clf.predict(X_part)\n",
    "            print(d.get(i))\n",
    "            print('-ACC:',accuracy_score(Y_part,Y_pred))\n",
    "            print('-REC:',recall_score(Y_part,Y_pred))\n",
    "            print('-F-Score:',f1_score(Y_part,Y_pred))\n",
    "            \"\"\"\n",
    "            dot_data=export_graphviz(clf,out_file=None,\n",
    "                                     feature_names=f_names,\n",
    "                                     class_names=['NL','L'],\n",
    "                                     filled=True,\n",
    "                                     rounded=True,\n",
    "                                     special_characters=True)\n",
    "            graph=pydotplus.graph_from_dot_data(dot_data)\n",
    "            graph.write_pdf('dt_tree.pdf')\n",
    "            \"\"\"\n",
    "            \"\"\"\n",
    "            # 以上画决策树这一段用上StringIO可以这么写（与上段等价）：\n",
    "            dot_data=StringIO()\n",
    "            export_graphviz(clf,out_file=dot_data,\n",
    "                            feature_names=f_names,\n",
    "                            class_names=['NL','L'],\n",
    "                            filled=True,\n",
    "                            rounded=True,\n",
    "                            special_characters=True)\n",
    "            graph=pydotplus.graph_from_dot_data(dot_data.getvalue())\n",
    "            graph.write_pdf('dt_tree.pdf')\n",
    "            \"\"\"\n",
    "            \n",
    "\n",
    "fetures,label=hr_preprocessing(dp=False)\n",
    "hr_modeling(fetures,label)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py3.6]",
   "language": "python",
   "name": "conda-env-py3.6-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
